# PRM_model.py
from typing import List, Optional, Tuple
import torch
import torch.nn.functional as F
from torch import nn
import os
import math


# å‡è®¾ .residual åŒ…å« DownBlock2d å’Œ UpBlock2dã€‚
# ä¸ºäº†ç‹¬ç«‹æµ‹è¯•ï¼Œå¦‚æœä¸å¯ç”¨ï¼Œæˆ‘ä»¬å®šä¹‰å¢å¼ºç‰ˆæœ¬ã€‚
try:
    from .residual import DownBlock2d, UpBlock2d
except ImportError:

    # --- SEæ³¨æ„åŠ›æ¨¡å— ---
    class SEBlock(nn.Module):
        """Squeeze-and-Excitation Block"""
        def __init__(self, channels, reduction=16):
            super().__init__()
            self.squeeze = nn.AdaptiveAvgPool2d(1)
            self.excitation = nn.Sequential(
                nn.Linear(channels, channels // reduction, bias=False),
                nn.ReLU(inplace=True),
                nn.Linear(channels // reduction, channels, bias=False),
                nn.Sigmoid()
            )

        def forward(self, x):
            b, c = x.size(0), x.size(1)
            y = self.squeeze(x).view(b, c)
            y = self.excitation(y).view(b, c, 1, 1)
            return x * y

    # --- å¢å¼ºæ®‹å·®å— ---
    class EnhancedResidualBlock(nn.Module):
        """å¢å¼ºçš„æ®‹å·®å—ï¼ŒåŒ…å«SEæ³¨æ„åŠ›"""
        def __init__(self, channels, num_layers=2, use_se=True):
            super().__init__()
            self.use_se = use_se

            layers = []
            for i in range(num_layers):
                layers.extend([
                    nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False),
                    nn.BatchNorm2d(channels),
                ])
                if i < num_layers - 1:
                    layers.append(nn.ReLU(inplace=True))

            self.conv_layers = nn.Sequential(*layers)

            if self.use_se:
                self.se = SEBlock(channels)

            self.final_relu = nn.ReLU(inplace=True)

        def forward(self, x):
            identity = x
            out = self.conv_layers(x)

            if self.use_se:
                out = self.se(out)

            out = out + identity
            out = self.final_relu(out)
            return out

    class DownBlock2d(nn.Module):
        """ä¸‹é‡‡æ ·å—"""
        def __init__(self, in_channels, out_channels, num_layers, downsample=False, use_se=True):
            super().__init__()
            self.downsample = downsample
            stride = 2 if downsample else 1

            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
            self.bn = nn.BatchNorm2d(out_channels)
            self.relu = nn.ReLU(inplace=True)

            self.residual_blocks = nn.ModuleList([
                EnhancedResidualBlock(out_channels, 2, use_se) for _ in range(num_layers)
            ])

            self.shortcut = nn.Identity()
            if stride != 1 or in_channels != out_channels:
                self.shortcut = nn.Sequential(
                    nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                    nn.BatchNorm2d(out_channels)
                )

        def forward(self, x):
            identity = self.shortcut(x)
            out = self.conv(x)
            out = self.bn(out)
            out = self.relu(out)

            if self.downsample and identity.shape[2:] != out.shape[2:]:
                identity = F.adaptive_avg_pool2d(identity, out.shape[2:])

            out = out + identity

            for res_block in self.residual_blocks:
                out = res_block(out)

            return out

    class UpBlock2d(nn.Module):
        """å¢å¼ºçš„ä¸Šé‡‡æ ·å—ï¼Œæ”¯æŒè·³è·ƒè¿æ¥"""
        def __init__(self, in_channels, out_channels, num_layers, upsample=False, skip_channels=0):
            super().__init__()
            self.upsample_layer = None
            if upsample:
                self.upsample_layer = nn.ConvTranspose2d(
                    in_channels, in_channels, kernel_size=4, stride=2, padding=1, bias=False
                )
                self.upsample_bn = nn.BatchNorm2d(in_channels)
                self.upsample_relu = nn.ReLU(inplace=True)

            conv_in_channels = in_channels + skip_channels
            self.conv = nn.Conv2d(conv_in_channels, out_channels, kernel_size=3, padding=1, bias=False)
            self.bn = nn.BatchNorm2d(out_channels)
            self.relu = nn.ReLU(inplace=True)

            self.residual_blocks = nn.ModuleList([
                EnhancedResidualBlock(out_channels, 2) for _ in range(num_layers)
            ])

        def forward(self, x, skip=None):
            if self.upsample_layer:
                x = self.upsample_layer(x)
                x = self.upsample_bn(x)
                x = self.upsample_relu(x)

            if skip is not None:
                if x.shape[2:] != skip.shape[2:]:
                    skip = F.interpolate(skip, size=x.shape[2:], mode='bilinear', align_corners=False)
                x = torch.cat([x, skip], dim=1)

            x = self.conv(x)
            x = self.bn(x)
            x = self.relu(x)

            for res_block in self.residual_blocks:
                x = res_block(x)

            return x


# --- åŠ¨ä½œç¼–ç å™¨ ---
class ActionEncoder(nn.Module):
    """åŠ¨ä½œç¼–ç å™¨ï¼Œå°†0-18çš„æ•´æ•°åŠ¨ä½œç¼–ç ä¸ºå‘é‡"""
    def __init__(self, action_dim=19, embed_dim=128, output_dim=256):  # 0-18å…±19ä¸ªåŠ¨ä½œ
        super().__init__()
        self.action_embedding = nn.Embedding(action_dim, embed_dim)
        self.action_proj = nn.Sequential(
            nn.Linear(embed_dim, output_dim),
            nn.LayerNorm(output_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1)
        )
        
    def forward(self, actions):
        """
        Args:
            actions: (B,) åŠ¨ä½œç´¢å¼•
        Returns:
            action_features: (B, output_dim) åŠ¨ä½œç‰¹å¾
        """
        embedded = self.action_embedding(actions.long())  # (B, embed_dim)
        return self.action_proj(embedded)  # (B, output_dim)


# --- Transformerå±‚ ---
class TransformerLayer(nn.Module):
    """å•ä¸ªTransformerå±‚ï¼Œä½¿ç”¨åŠ¨ä½œä½œä¸ºqueryçš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(self, d_model=256, n_heads=8, d_ff=1024, dropout=0.1):
        super().__init__()
        assert d_model % n_heads == 0, "d_model must be divisible by n_heads"
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        self.scale = 1.0 / math.sqrt(self.head_dim)
        
        # Multi-head attention projections
        self.q_proj = nn.Linear(d_model, d_model, bias=False)
        self.k_proj = nn.Linear(d_model, d_model, bias=False)
        self.v_proj = nn.Linear(d_model, d_model, bias=False)
        self.out_proj = nn.Linear(d_model, d_model)
        
        # Feed forward network
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        
        # Layer normalization
        self.ln1 = nn.LayerNorm(d_model)
        self.ln2 = nn.LayerNorm(d_model)
        
        # Dropout
        self.attn_dropout = nn.Dropout(dropout)
        self.ffn_dropout = nn.Dropout(dropout)
        
    def forward(self, x, action_features):
        """
        Args:
            x: è§†è§‰ç‰¹å¾ (B, d_model)
            action_features: åŠ¨ä½œç‰¹å¾ (B, d_model)
        Returns:
            è¾“å‡ºç‰¹å¾ (B, d_model)
        """
        B = x.size(0)
        
        # 1. Multi-head attention with residual connection
        residual = x
        x_norm = self.ln1(x)
        action_norm = self.ln1(action_features)
        
        # åŠ¨ä½œä½œä¸ºquery, è§†è§‰ç‰¹å¾ä½œä¸ºkeyå’Œvalue
        Q = self.q_proj(action_norm).view(B, self.n_heads, self.head_dim)  # (B, H, D)
        K = self.k_proj(x_norm).view(B, self.n_heads, self.head_dim)       # (B, H, D)
        V = self.v_proj(x_norm).view(B, self.n_heads, self.head_dim)       # (B, H, D)
        
        # æ³¨æ„åŠ›è®¡ç®—
        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale  # (B, H, 1)
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.attn_dropout(attn_weights)
        
        # åº”ç”¨æ³¨æ„åŠ›
        attended = torch.matmul(attn_weights, V)  # (B, H, D)
        attended = attended.view(B, self.d_model)  # (B, d_model)
        
        # è¾“å‡ºæŠ•å½±å’Œæ®‹å·®è¿æ¥
        attended = self.out_proj(attended)
        x = residual + attended
        
        # 2. Feed forward with residual connection
        residual = x
        x_norm = self.ln2(x)
        x = residual + self.ffn_dropout(self.ffn(x_norm))
        
        return x


# --- ä¸‰å±‚Transformeré¢„æµ‹æ¨¡å‹ ---
class PredictiveModel(nn.Module):
    """ä¸‰å±‚Transformeré¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, d_model=256, n_heads=8, n_layers=3, d_ff=1024, dropout=0.1, action_dim=19):
        super().__init__()
        
        # åŠ¨ä½œç¼–ç å™¨
        self.action_encoder = ActionEncoder(action_dim=action_dim, output_dim=d_model)
        
        # Transformerå±‚
        self.layers = nn.ModuleList([
            TransformerLayer(d_model, n_heads, d_ff, dropout) 
            for _ in range(n_layers)
        ])
        
        # è¾“å‡ºå±‚å½’ä¸€åŒ–
        self.output_ln = nn.LayerNorm(d_model)
        
    def forward(self, x, actions):
        """
        Args:
            x: ç¼–ç å™¨è¾“å‡º (B, d_model)
            actions: åŠ¨ä½œç´¢å¼• (B,)
        Returns:
            é¢„æµ‹ç‰¹å¾ (B, d_model)
        """
        # ç¼–ç åŠ¨ä½œ
        action_features = self.action_encoder(actions)  # (B, d_model)
        
        # é€šè¿‡Transformerå±‚
        for layer in self.layers:
            x = layer(x, action_features)
        
        # è¾“å‡ºå½’ä¸€åŒ–
        x = self.output_ln(x)
        
        return x


# --- ç¼–ç å™¨ï¼ˆé‡å‘½åä¸ºencoderï¼‰---
class Encoder(nn.Module):
    """ç¼–ç å™¨ï¼Œè¾“å…¥210*160*3å›¾åƒ"""

    def __init__(self, in_channels: int = 3, layers: List[int] = [2, 3, 4, 3], base_channels: int = 64) -> None:
        super().__init__()

        # Stemå±‚ - å¤„ç†210*160*3è¾“å…¥
        self.stem = nn.Sequential(
            nn.Conv2d(in_channels, base_channels // 2, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(base_channels // 2),
            nn.ReLU(inplace=True),
            nn.Conv2d(base_channels // 2, base_channels, kernel_size=3, stride=2, padding=1, bias=False),  # 105*80
            nn.BatchNorm2d(base_channels),
            nn.ReLU(inplace=True),
        )

        # ç‰¹å¾æå–ç½‘ç»œ
        self.layer1 = DownBlock2d(base_channels, base_channels, layers[0], downsample=False)       # 105*80
        self.layer2 = DownBlock2d(base_channels, base_channels * 2, layers[1], downsample=True)    # 53*40
        self.layer3 = DownBlock2d(base_channels * 2, base_channels * 4, layers[2], downsample=True) # 27*20
        self.layer4 = DownBlock2d(base_channels * 4, base_channels * 8, layers[3], downsample=True) # 14*10

        # å…¨å±€å¹³å‡æ± åŒ–
        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))

        # ç‰¹å¾æŠ•å½±
        self.feature_proj = nn.Sequential(
            nn.Linear(base_channels * 8, 512),
            nn.LayerNorm(512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.LayerNorm(256)
        )

        self.skip_features = []

    def forward(self, x: torch.Tensor) -> tuple:
        """
        Args:
            x: è¾“å…¥å›¾åƒ (B, 3, 210, 160)
        Returns:
            features: ç‰¹å¾å‘é‡ (B, 256)
            skip_features: è·³è·ƒè¿æ¥ç‰¹å¾åˆ—è¡¨
        """
        # ç¡®ä¿è¾“å…¥å°ºå¯¸æ­£ç¡®
        assert x.shape[-2:] == (210, 160), f"Expected input size (210, 160), got {x.shape[-2:]}"
        
        self.skip_features = []

        x = self.stem(x)  # (B, 64, 105, 80)
        self.skip_features.append(x)

        x = self.layer1(x)  # (B, 64, 105, 80)
        self.skip_features.append(x)

        x = self.layer2(x)  # (B, 128, 53, 40)
        self.skip_features.append(x)

        x = self.layer3(x)  # (B, 256, 27, 20)
        self.skip_features.append(x)

        x = self.layer4(x)  # (B, 512, 14, 10)

        # å…¨å±€æ± åŒ–å’Œç‰¹å¾æŠ•å½±
        pooled = self.global_pool(x)  # (B, 512, 1, 1)
        flattened = pooled.view(pooled.size(0), -1)  # (B, 512)
        features = self.feature_proj(flattened)  # (B, 256)

        return features, self.skip_features


# --- å›¾åƒè§£ç å™¨ï¼ˆè¾“å‡º[0,1]èŒƒå›´ï¼‰---
class ImageDecoder(nn.Module):
    """å›¾åƒè§£ç å™¨ï¼Œè¾“å‡º210*160*3å›¾åƒï¼ŒèŒƒå›´[0,1]"""

    def __init__(self, latent_dim: int = 256, out_channels: int = 3, layers: List[int] = [2, 2, 2, 1],
                 base_channels: int = 64, use_skip_connections: bool = True) -> None:
        super().__init__()
        self.use_skip_connections = use_skip_connections
        self.target_size = (210, 160)

        # åˆå§‹æŠ•å½± - æŠ•å½±åˆ°14*10ç‰¹å¾å›¾
        self.initial_size = (14, 10)  # ä¸encoderæœ€åä¸€å±‚å¯¹åº”
        self.initial_channels = base_channels * 8  # 512
        self.initial_proj = nn.Sequential(
            nn.Linear(latent_dim, self.initial_channels * self.initial_size[0] * self.initial_size[1]),
            nn.LayerNorm(self.initial_channels * self.initial_size[0] * self.initial_size[1]),
            nn.ReLU(inplace=True)
        )

        # ä¸Šé‡‡æ ·å—
        skip_channels = [256, 128, 64, 64] if use_skip_connections else [0, 0, 0, 0]

        self.up1 = UpBlock2d(self.initial_channels, base_channels * 4, layers[0],
                            upsample=True, skip_channels=skip_channels[0])  # 27*20
        self.up2 = UpBlock2d(base_channels * 4, base_channels * 2, layers[1],
                            upsample=True, skip_channels=skip_channels[1])  # 53*40
        self.up3 = UpBlock2d(base_channels * 2, base_channels, layers[2],
                            upsample=True, skip_channels=skip_channels[2])  # 105*80
        self.up4 = UpBlock2d(base_channels, base_channels, layers[3] if len(layers) > 3 else 1,
                            upsample=True, skip_channels=skip_channels[3])  # 210*160

        # æœ€ç»ˆè¾“å‡ºå±‚ - ç¡®ä¿è¾“å‡º[0,1]èŒƒå›´
        self.final_conv = nn.Sequential(
            nn.Conv2d(base_channels, base_channels // 2, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(base_channels // 2),
            nn.ReLU(inplace=True),
            nn.Conv2d(base_channels // 2, out_channels, kernel_size=3, padding=1),
            nn.Sigmoid()  # ç¡®ä¿è¾“å‡º[0,1]
        )

    def forward(self, x: torch.Tensor, skip_features: Optional[List] = None) -> torch.Tensor:
        """
        Args:
            x: æ½œåœ¨ç‰¹å¾ (B, 256)
            skip_features: è·³è·ƒè¿æ¥ç‰¹å¾
        Returns:
            é‡å»ºå›¾åƒ (B, 3, 210, 160)
        """
        # æŠ•å½±åˆ°åˆå§‹ç‰¹å¾å›¾
        x = self.initial_proj(x)
        x = x.view(x.size(0), self.initial_channels, self.initial_size[0], self.initial_size[1])

        # ä¸Šé‡‡æ ·
        if self.use_skip_connections and skip_features is not None:
            skip_list = skip_features[::-1]  # å€’åº
            x = self.up1(x, skip_list[0] if len(skip_list) > 0 else None)
            x = self.up2(x, skip_list[1] if len(skip_list) > 1 else None)
            x = self.up3(x, skip_list[2] if len(skip_list) > 2 else None)
            x = self.up4(x, skip_list[3] if len(skip_list) > 3 else None)
        else:
            x = self.up1(x)
            x = self.up2(x)
            x = self.up3(x)
            x = self.up4(x)

        # æœ€ç»ˆå·ç§¯
        x = self.final_conv(x)

        # ç¡®ä¿è¾“å‡ºå°ºå¯¸æ­£ç¡®
        if x.shape[-2:] != self.target_size:
            x = F.interpolate(x, size=self.target_size, mode='bilinear', align_corners=False)

        return x


# --- Doneåˆ†ç±»å™¨ ---
class DoneClassifier(nn.Module):
    """Doneåˆ†ç±»å™¨ï¼Œåˆ¤æ–­æ¸¸æˆæ˜¯å¦ç»“æŸ"""
    
    def __init__(self, latent_dim=256, hidden_dim=512, dropout=0.1):
        super().__init__()
        
        self.classifier = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()  # è¾“å‡º0åˆ°1çš„æ¦‚ç‡
        )
        
    def forward(self, x):
        """
        Args:
            x: è¾“å…¥ç‰¹å¾ (B, latent_dim)
        Returns:
            doneé¢„æµ‹ (B, 1) - 0åˆ°1çš„æ¦‚ç‡
        """
        return self.classifier(x)


# --- PRMä¸»æ¨¡å‹ ---
class PRM(nn.Module):
    """Predictive Representation Model"""
    
    def __init__(
        self, 
        img_in_channels: int = 3,
        img_out_channels: int = 3,
        encoder_layers: List[int] = [2, 3, 4, 3], 
        decoder_layers: List[int] = [2, 2, 2, 1],
        action_dim: int = 19,  # 0-18å…±19ä¸ªåŠ¨ä½œ
        latent_dim: int = 256,
        base_channels: int = 64, 
        num_attention_heads: int = 8,
        transformer_layers: int = 3,
        use_skip_connections: bool = True,
        loss_weights: List[float] = [1.0, 1.0, 1.0],  # [w1, w2, w3]
        dropout: float = 0.1
    ) -> None:
        super().__init__()
        
        # ç¼–ç å™¨
        self.encoder = Encoder(img_in_channels, encoder_layers, base_channels)
        
        # ä¸‰å±‚Transformeré¢„æµ‹æ¨¡å‹
        self.predictive_model = PredictiveModel(
            d_model=latent_dim,
            n_heads=num_attention_heads,
            n_layers=transformer_layers,
            action_dim=action_dim,
            dropout=dropout
        )
        
        # å›¾åƒè§£ç å™¨
        self.image_decoder = ImageDecoder(
            latent_dim, img_out_channels, decoder_layers,
            base_channels, use_skip_connections
        )
        
        # Doneåˆ†ç±»å™¨
        self.done_classifier = DoneClassifier(latent_dim, dropout=dropout)
        
        self.use_skip_connections = use_skip_connections
        self.loss_weights = loss_weights
        
        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)
        
    def _init_weights(self, m):
        """å‚è€ƒLLMçš„æƒé‡åˆå§‹åŒ–ç­–ç•¥"""
        if isinstance(m, nn.Linear):
            # Xavier uniform for linear layers
            nn.init.xavier_uniform_(m.weight)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.Embedding):
            # æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–embedding
            nn.init.normal_(m.weight, mean=0, std=0.02)
        elif isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
            # Kaimingåˆå§‹åŒ–å·ç§¯å±‚
            nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, (nn.BatchNorm2d, nn.LayerNorm)):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
    
    def forward(self, images: torch.Tensor, actions: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        Args:
            images: è¾“å…¥å›¾åƒ (B, 3, 210, 160)
            actions: åŠ¨ä½œ (B,) - 0åˆ°18çš„æ•´æ•°
        Returns:
            encoder_features: ç¼–ç å™¨è¾“å‡º (B, 256)
            predicted_features: é¢„æµ‹æ¨¡å‹è¾“å‡º (B, 256)
            reconstructed_images: é‡å»ºå›¾åƒ (B, 3, 210, 160)
            done_predictions: doneé¢„æµ‹ (B, 1)
        """
        # ç¡®ä¿åŠ¨ä½œåœ¨æ­£ç¡®èŒƒå›´å†…
        assert actions.min() >= 0 and actions.max() <= 18, f"Actions must be in range [0, 18], got [{actions.min()}, {actions.max()}]"
        
        # ç¼–ç 
        encoder_features, skip_features = self.encoder(images)  # (B, 256)
        
        # Transformeré¢„æµ‹
        predicted_features = self.predictive_model(encoder_features, actions)  # (B, 256)
        
        # è§£ç 
        if self.use_skip_connections:
            reconstructed_images = self.image_decoder(predicted_features, skip_features)
        else:
            reconstructed_images = self.image_decoder(predicted_features)
            
        done_predictions = self.done_classifier(predicted_features)
        
        return encoder_features, predicted_features, reconstructed_images, done_predictions
    
    def compute_loss(self, 
                predicted_images: torch.Tensor,
                target_images: torch.Tensor,
                predicted_done: torch.Tensor,
                target_done: torch.Tensor,
                predicted_features: torch.Tensor,
                target_features: torch.Tensor) -> Tuple[torch.Tensor, dict]:
        """
        è®¡ç®—ç»¼åˆæŸå¤±
        Args:
            predicted_images: é¢„æµ‹å›¾åƒ (B, 3, 210, 160)
            target_images: ç›®æ ‡å›¾åƒ (B, 3, 210, 160)
            predicted_done: é¢„æµ‹doneå€¼ (B, 1)
            target_done: çœŸå®doneå€¼ (B,)
            predicted_features: é¢„æµ‹ç‰¹å¾ (B, 256)
            target_features: ç›®æ ‡ç‰¹å¾ (B, 256)
        """
        # ç¡®ä¿ç›®æ ‡å›¾åƒåœ¨[0,1]èŒƒå›´å†…
        if target_images.min() < 0:
            target_images = (target_images + 1) / 2.0
        
        # Loss_img: MSEæŸå¤± - æ‰¹æ¬¡å†…å¹³å‡
        loss_img = F.mse_loss(predicted_images, target_images, reduction='mean')
        
        # Loss_done: äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤± - æ‰¹æ¬¡å†…å¹³å‡
        target_done_float = target_done.float().view(-1, 1)
        loss_done = F.binary_cross_entropy(predicted_done, target_done_float, reduction='mean')
        
        # Loss_vector: ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤± - æ‰¹æ¬¡å†…å¹³å‡
        pred_norm = F.normalize(predicted_features, p=2, dim=1)
        target_norm = F.normalize(target_features, p=2, dim=1)
        cosine_sim = (pred_norm * target_norm).sum(dim=1)
        loss_vector = (1.0 - cosine_sim).mean()  # ä½™å¼¦è·ç¦»çš„å¹³å‡
        
        # æ€»æŸå¤±
        w1, w2, w3 = self.loss_weights
        total_loss = w1 * loss_img + w2 * loss_done + w3 * loss_vector
        
        loss_dict = {
            'total_loss': total_loss.item(),
            'loss_img': loss_img.item(),
            'loss_done': loss_done.item(),
            'loss_vector': loss_vector.item(),
            'cosine_similarity': cosine_sim.mean().item()
        }
        
        return total_loss, loss_dict

def train_on_batch(self, 
                  batch_images: torch.Tensor,
                  batch_actions: torch.Tensor,
                  batch_next_images: torch.Tensor,
                  batch_done: torch.Tensor,
                  optimizer: torch.optim.Optimizer) -> dict:
    """
    åœ¨å•ä¸ªbatchä¸Šè®­ç»ƒ
    Args:
        batch_images: å½“å‰å›¾åƒ [I1,I2,I3...] (B, 3, 210, 160)
        batch_actions: åŠ¨ä½œ (B,)
        batch_next_images: ä¸‹ä¸€å¸§å›¾åƒ (B, 3, 210, 160) 
        batch_done: doneæ ‡ç­¾ (B,)
        optimizer: ä¼˜åŒ–å™¨
    """
    self.train()
    optimizer.zero_grad()
    
    batch_size = batch_images.size(0)
    if batch_size <= 1:
        raise ValueError("Batch size must be greater than 1")
    
    # æŒ‰ç…§è¦æ±‚ï¼šå¯¹batchä¸­å‰mä¸ªæ ·æœ¬è¿›è¡Œå¤„ç† (1<=m<=batch_size-1)
    # ä½¿ç”¨I1,I2,...,Imé¢„æµ‹I2',I3',...,I(m+1)'
    input_images = batch_images[:-1]    # [I1,I2,...,Im]
    input_actions = batch_actions[:-1]  # å¯¹åº”çš„åŠ¨ä½œ
    target_next_images = batch_next_images[:-1]  # [I2,I3,...,I(m+1)] - çœŸå®çš„ä¸‹ä¸€å¸§
    target_done = batch_done[1:]        # [done2,done3,...,done(m+1)] - çœŸå®çš„doneå€¼
    
    # å‰å‘ä¼ æ’­ï¼šç”¨[I1,I2,...,Im]å’Œå¯¹åº”åŠ¨ä½œé¢„æµ‹
    encoder_features, predicted_features, predicted_images, predicted_done = self.forward(
        input_images, input_actions
    )
    
    # è·å–ç›®æ ‡encoderç‰¹å¾ - å¯¹çœŸå®çš„ä¸‹ä¸€å¸§å›¾åƒç¼–ç 
    with torch.no_grad():
        target_encoder_features, _ = self.encoder(batch_images[1:])  # [x2,x3,...,x(m+1)]
    
    # ä½¿ç”¨ç»Ÿä¸€çš„compute_losså‡½æ•°è®¡ç®—æŸå¤±
    total_loss, loss_dict = self.compute_loss(
        predicted_images,
        target_next_images,
        predicted_done,
        target_done,
        predicted_features,
        target_encoder_features
    )
    
    # åå‘ä¼ æ’­å’Œä¼˜åŒ–
    total_loss.backward()
    torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)
    optimizer.step()
    
    return loss_dict

def evaluate_on_batch(self,
                     batch_images: torch.Tensor,
                     batch_actions: torch.Tensor, 
                     batch_next_images: torch.Tensor,
                     batch_done: torch.Tensor) -> dict:
    """
    åœ¨å•ä¸ªbatchä¸ŠéªŒè¯
    Args:
        batch_images: å½“å‰å›¾åƒ [I1,I2,I3...] (B, 3, 210, 160)
        batch_actions: åŠ¨ä½œ (B,)
        batch_next_images: ä¸‹ä¸€å¸§å›¾åƒ (B, 3, 210, 160)
        batch_done: doneæ ‡ç­¾ (B,)
    Returns:
        loss_dict: åŒ…å«å„é¡¹æŸå¤±çš„å­—å…¸
    """
    self.eval()
    
    batch_size = batch_images.size(0)
    if batch_size <= 1:
        raise ValueError("Batch size must be greater than 1")
    
    with torch.no_grad():
        # æŒ‰ç…§è¦æ±‚ï¼šå¯¹batchä¸­å‰mä¸ªæ ·æœ¬è¿›è¡Œå¤„ç† (1<=m<=batch_size-1)
        input_images = batch_images[:-1]    # [I1,I2,...,Im]
        input_actions = batch_actions[:-1]  # å¯¹åº”çš„åŠ¨ä½œ
        target_next_images = batch_next_images[:-1]  # [I2,I3,...,I(m+1)] 
        target_done = batch_done[1:]        # [done2,done3,...,done(m+1)]
        
        # å‰å‘ä¼ æ’­
        encoder_features, predicted_features, predicted_images, predicted_done = self.forward(
            input_images, input_actions
        )
        
        # è·å–ç›®æ ‡encoderç‰¹å¾
        target_encoder_features, _ = self.encoder(batch_images[1:])  # [x2,x3,...,x(m+1)]
        
        # ä½¿ç”¨ç»Ÿä¸€çš„compute_losså‡½æ•°è®¡ç®—æŸå¤±
        total_loss, loss_dict = self.compute_loss(
            predicted_images,
            target_next_images,
            predicted_done,
            target_done,
            predicted_features,
            target_encoder_features
        )
        
        # è®¡ç®—é¢å¤–çš„è¯„ä¼°æŒ‡æ ‡
        # Doneé¢„æµ‹å‡†ç¡®ç‡
        done_pred_binary = (predicted_done > 0.5).float()
        done_accuracy = (done_pred_binary.view(-1) == target_done.float()).float().mean()
        
        # å›¾åƒé‡å»ºPSNR
        mse_img = F.mse_loss(predicted_images, target_next_images, reduction='mean')
        psnr = 20 * torch.log10(1.0 / torch.sqrt(mse_img + 1e-8))
        
        # æ·»åŠ é¢å¤–è¯„ä¼°æŒ‡æ ‡åˆ°loss_dict
        loss_dict.update({
            'done_accuracy': done_accuracy.item(),
            'psnr': psnr.item(),
            'mse_img': mse_img.item()
        })
        
    return loss_dict
def create_optimized_optimizer(model, base_lr=1e-4, weight_decay=1e-2):
    """
    åˆ›å»ºä¼˜åŒ–çš„ä¼˜åŒ–å™¨é…ç½®ï¼Œå‚è€ƒLLMè®­ç»ƒæœ€ä½³å®è·µ
    """
    # åˆ†ç¦»ä¸åŒç±»å‹çš„å‚æ•°
    decay_params = []
    no_decay_params = []
    
    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue
        # LayerNorm, BatchNorm, biasç­‰ä¸è¿›è¡Œæƒé‡è¡°å‡
        if len(param.shape) <= 1 or name.endswith(".bias") or "norm" in name.lower():
            no_decay_params.append(param)
        else:
            decay_params.append(param)
    
    optimizer_grouped_parameters = [
        {
            "params": decay_params,
            "weight_decay": weight_decay,
            "lr": base_lr
        },
        {
            "params": no_decay_params,
            "weight_decay": 0.0,
            "lr": base_lr
        }
    ]
    
    # ä½¿ç”¨AdamWä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        optimizer_grouped_parameters,
        lr=base_lr,
        betas=(0.9, 0.95),  # å‚è€ƒLLMè®­ç»ƒçš„betaå€¼
        eps=1e-8,
        weight_decay=weight_decay
    )
    
    return optimizer


def create_lr_scheduler(optimizer, warmup_steps=1000, max_steps=10000, min_lr_ratio=0.1):
    """
    åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ŒåŒ…å«warmupå’Œcosine annealing
    """
    def lr_lambda(step):
        if step < warmup_steps:
            # Warmupé˜¶æ®µ
            return step / warmup_steps
        else:
            # Cosine annealingé˜¶æ®µ
            progress = (step - warmup_steps) / (max_steps - warmup_steps)
            progress = min(progress, 1.0)
            cosine_decay = 0.5 * (1 + math.cos(math.pi * progress))
            return min_lr_ratio + (1 - min_lr_ratio) * cosine_decay
    
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    return scheduler


if __name__ == '__main__':
    # --- æ¨¡å‹æµ‹è¯• ---
    BATCH_SIZE = 8
    IMG_C = 3
    IMG_H = 210
    IMG_W = 160
    ACTION_DIM = 19  # 0-18å…±19ä¸ªåŠ¨ä½œ

    print("=== PRM Model Testing ===")
    print(f"Input image size: {IMG_H}x{IMG_W}x{IMG_C}")
    print(f"Action range: 0-{ACTION_DIM-1}")
    print(f"Batch size: {BATCH_SIZE}")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    dummy_images = torch.rand(BATCH_SIZE, IMG_C, IMG_H, IMG_W)  # [0,1]èŒƒå›´
    dummy_actions = torch.randint(0, ACTION_DIM, (BATCH_SIZE,))  # 0-18çš„åŠ¨ä½œ
    dummy_next_images = torch.rand(BATCH_SIZE, IMG_C, IMG_H, IMG_W)  # [0,1]èŒƒå›´
    dummy_done = torch.randint(0, 2, (BATCH_SIZE,))  # 0æˆ–1

    # åˆ›å»ºæ¨¡å‹
    model = PRM(
        img_in_channels=IMG_C,
        img_out_channels=IMG_C,
        encoder_layers=[2, 3, 4, 3],
        decoder_layers=[2, 2, 2, 1],
        action_dim=ACTION_DIM,
        latent_dim=256,
        base_channels=64,
        num_attention_heads=8,
        transformer_layers=3,
        use_skip_connections=True,
        loss_weights=[1.0, 0.5, 0.3],
        dropout=0.1
    )

    # å‚æ•°ç»Ÿè®¡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\næ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")

    # æµ‹è¯•å‰å‘ä¼ æ’­
    print(f"\n=== å‰å‘ä¼ æ’­æµ‹è¯• ===")
    model.eval()
    with torch.no_grad():
        encoder_features, predicted_features, reconstructed_images, done_predictions = model(
            dummy_images, dummy_actions
        )
        print(f"âœ… ç¼–ç å™¨ç‰¹å¾: {encoder_features.shape}")
        print(f"âœ… é¢„æµ‹ç‰¹å¾: {predicted_features.shape}")
        print(f"âœ… é‡å»ºå›¾åƒ: {reconstructed_images.shape}")
        print(f"âœ… Doneé¢„æµ‹: {done_predictions.shape}")

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = create_optimized_optimizer(model, base_lr=1e-4, weight_decay=1e-2)
    scheduler = create_lr_scheduler(optimizer, warmup_steps=100, max_steps=1000)

    # æµ‹è¯•è®­ç»ƒ
    print(f"\n=== è®­ç»ƒæµ‹è¯• ===")
    n_epochs = 50
    
    for epoch in range(n_epochs):
        # è®­ç»ƒ
        loss_dict = model.train_on_batch(
            dummy_images, dummy_actions, dummy_next_images, dummy_done, optimizer
        )
        scheduler.step()
        
        if (epoch + 1) % 10 == 0:
            current_lr = optimizer.param_groups[0]['lr']
            print(f'Epoch {epoch + 1:3d}/{n_epochs}, '
                  f'Total: {loss_dict["total_loss"]:.6f}, '
                  f'LR: {current_lr:.2e}')
            
            # éªŒè¯
            val_loss_dict = model.evaluate_on_batch(
                dummy_images, dummy_actions, dummy_next_images, dummy_done
            )
            print(f'    Validation: {val_loss_dict["total_loss"]:.6f}')

    print(f"\n=== PRMæ¨¡å‹æ€»ç»“ ===")
    print(f"âœ… ç¼–ç å™¨: å¤„ç†210Ã—160Ã—3å›¾åƒ")
    print(f"âœ… åŠ¨ä½œç¼–ç : 0-18æ•´æ•°åŠ¨ä½œembedding") 
    print(f"âœ… æ³¨æ„åŠ›: åŠ¨ä½œä½œä¸ºqueryï¼Œè§†è§‰ç‰¹å¾ä½œä¸ºkey/value")
    print(f"âœ… è§£ç å™¨: å›¾åƒé‡å»º[0,1] + Doneåˆ†ç±»[0,1]")
    print(f"âœ… ä¼˜åŒ–: AdamW + warmup + ä½™å¼¦é€€ç«")
    print(f"âœ… æ€»å‚æ•°: {total_params:,}")
    print("ğŸ‰ PRMæ¨¡å‹æµ‹è¯•å®Œæˆï¼")
            