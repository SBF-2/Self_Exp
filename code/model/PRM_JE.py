# PRM_JE.py - Simplified PRM with only Encoder and Predictive Model
from typing import List, Optional, Tuple
import torch
import torch.nn.functional as F
from torch import nn
import os
import math


# å‡è®¾ .residual åŒ…å« DownBlock2d å’Œ UpBlock2dã€‚
# ä¸ºäº†ç‹¬ç«‹æµ‹è¯•ï¼Œå¦‚æœä¸å¯ç”¨ï¼Œæˆ‘ä»¬å®šä¹‰å¢å¼ºç‰ˆæœ¬ã€‚
try:
    from .residual import DownBlock2d, UpBlock2d
except ImportError:

    # --- SEæ³¨æ„åŠ›æ¨¡å— ---
    class SEBlock(nn.Module):
        """Squeeze-and-Excitation Block"""
        def __init__(self, channels, reduction=16):
            super().__init__()
            self.squeeze = nn.AdaptiveAvgPool2d(1)
            self.excitation = nn.Sequential(
                nn.Linear(channels, channels // reduction, bias=False),
                nn.ReLU(inplace=True),
                nn.Linear(channels // reduction, channels, bias=False),
                nn.Sigmoid()
            )

        def forward(self, x):
            b, c = x.size(0), x.size(1)
            y = self.squeeze(x).view(b, c)
            y = self.excitation(y).view(b, c, 1, 1)
            return x * y

    # --- å¢å¼ºæ®‹å·®å— ---
    class EnhancedResidualBlock(nn.Module):
        """å¢å¼ºçš„æ®‹å·®å—ï¼ŒåŒ…å«SEæ³¨æ„åŠ›"""
        def __init__(self, channels, num_layers=2, use_se=True):
            super().__init__()
            self.use_se = use_se

            layers = []
            for i in range(num_layers):
                layers.extend([
                    nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False),
                    nn.BatchNorm2d(channels),
                ])
                if i < num_layers - 1:
                    layers.append(nn.ReLU(inplace=True))

            self.conv_layers = nn.Sequential(*layers)

            if self.use_se:
                self.se = SEBlock(channels)

            self.final_relu = nn.ReLU(inplace=True)

        def forward(self, x):
            identity = x
            out = self.conv_layers(x)

            if self.use_se:
                out = self.se(out)

            out = out + identity
            out = self.final_relu(out)
            return out

    class DownBlock2d(nn.Module):
        """ä¸‹é‡‡æ ·å—"""
        def __init__(self, in_channels, out_channels, num_layers, downsample=False, use_se=True):
            super().__init__()
            self.downsample = downsample
            stride = 2 if downsample else 1

            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
            self.bn = nn.BatchNorm2d(out_channels)
            self.relu = nn.ReLU(inplace=True)

            self.residual_blocks = nn.ModuleList([
                EnhancedResidualBlock(out_channels, 2, use_se) for _ in range(num_layers)
            ])

            self.shortcut = nn.Identity()
            if stride != 1 or in_channels != out_channels:
                self.shortcut = nn.Sequential(
                    nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                    nn.BatchNorm2d(out_channels)
                )

        def forward(self, x):
            identity = self.shortcut(x)
            out = self.conv(x)
            out = self.bn(out)
            out = self.relu(out)

            if self.downsample and identity.shape[2:] != out.shape[2:]:
                identity = F.adaptive_avg_pool2d(identity, out.shape[2:])

            out = out + identity

            for res_block in self.residual_blocks:
                out = res_block(out)

            return out


# --- åŠ¨ä½œç¼–ç å™¨ ---
class ActionEncoder(nn.Module):
    """åŠ¨ä½œç¼–ç å™¨ï¼Œå°†0-18çš„æ•´æ•°åŠ¨ä½œç¼–ç ä¸ºå‘é‡"""
    def __init__(self, action_dim=19, embed_dim=128, output_dim=256):  # 0-18å…±19ä¸ªåŠ¨ä½œ
        super().__init__()
        self.action_embedding = nn.Embedding(action_dim, embed_dim)
        self.action_proj = nn.Sequential(
            nn.Linear(embed_dim, output_dim),
            nn.LayerNorm(output_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1)
        )
        
    def forward(self, actions):
        """
        Args:
            actions: (B,) åŠ¨ä½œç´¢å¼•
        Returns:
            action_features: (B, output_dim) åŠ¨ä½œç‰¹å¾
        """
        embedded = self.action_embedding(actions.long())  # (B, embed_dim)
        return self.action_proj(embedded)  # (B, output_dim)


# --- Transformerå±‚ ---
class TransformerLayer(nn.Module):
    """å•ä¸ªTransformerå±‚ï¼Œä½¿ç”¨åŠ¨ä½œä½œä¸ºqueryçš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(self, d_model=256, n_heads=8, d_ff=1024, dropout=0.1):
        super().__init__()
        assert d_model % n_heads == 0, "d_model must be divisible by n_heads"
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        self.scale = 1.0 / math.sqrt(self.head_dim)
        
        # Multi-head attention projections
        self.q_proj = nn.Linear(d_model, d_model, bias=False)
        self.k_proj = nn.Linear(d_model, d_model, bias=False)
        self.v_proj = nn.Linear(d_model, d_model, bias=False)
        self.out_proj = nn.Linear(d_model, d_model)
        
        # Feed forward network
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        
        # Layer normalization
        self.ln1 = nn.LayerNorm(d_model)
        self.ln2 = nn.LayerNorm(d_model)
        
        # Dropout
        self.attn_dropout = nn.Dropout(dropout)
        self.ffn_dropout = nn.Dropout(dropout)
        
    def forward(self, x, action_features):
        """
        Args:
            x: è§†è§‰ç‰¹å¾ (B, d_model)
            action_features: åŠ¨ä½œç‰¹å¾ (B, d_model)
        Returns:
            è¾“å‡ºç‰¹å¾ (B, d_model)
        """
        B = x.size(0)
        
        # 1. Multi-head attention with residual connection
        residual = x
        x_norm = self.ln1(x)
        action_norm = self.ln1(action_features)
        
        # åŠ¨ä½œä½œä¸ºquery, è§†è§‰ç‰¹å¾ä½œä¸ºkeyå’Œvalue
        Q = self.q_proj(action_norm).view(B, self.n_heads, self.head_dim)  # (B, H, D)
        K = self.k_proj(x_norm).view(B, self.n_heads, self.head_dim)       # (B, H, D)
        V = self.v_proj(x_norm).view(B, self.n_heads, self.head_dim)       # (B, H, D)
        
        # æ³¨æ„åŠ›è®¡ç®—
        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale  # (B, H, 1)
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.attn_dropout(attn_weights)
        
        # åº”ç”¨æ³¨æ„åŠ›
        attended = torch.matmul(attn_weights, V)  # (B, H, D)
        attended = attended.view(B, self.d_model)  # (B, d_model)
        
        # è¾“å‡ºæŠ•å½±å’Œæ®‹å·®è¿æ¥
        attended = self.out_proj(attended)
        x = residual + attended
        
        # 2. Feed forward with residual connection
        residual = x
        x_norm = self.ln2(x)
        x = residual + self.ffn_dropout(self.ffn(x_norm))
        
        return x


# --- ä¸‰å±‚Transformeré¢„æµ‹æ¨¡å‹ ---
class PredictiveModel(nn.Module):
    """ä¸‰å±‚Transformeré¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, d_model=256, n_heads=8, n_layers=3, d_ff=1024, dropout=0.1, action_dim=19):
        super().__init__()
        
        # åŠ¨ä½œç¼–ç å™¨
        self.action_encoder = ActionEncoder(action_dim=action_dim, output_dim=d_model)
        
        # Transformerå±‚
        self.layers = nn.ModuleList([
            TransformerLayer(d_model, n_heads, d_ff, dropout) 
            for _ in range(n_layers)
        ])
        
        # è¾“å‡ºå±‚å½’ä¸€åŒ–
        self.output_ln = nn.LayerNorm(d_model)
        
    def forward(self, x, actions):
        """
        Args:
            x: ç¼–ç å™¨è¾“å‡º (B, d_model)
            actions: åŠ¨ä½œç´¢å¼• (B,)
        Returns:
            é¢„æµ‹ç‰¹å¾ (B, d_model)
        """
        # ç¼–ç åŠ¨ä½œ
        action_features = self.action_encoder(actions)  # (B, d_model)
        
        # é€šè¿‡Transformerå±‚
        for layer in self.layers:
            x = layer(x, action_features)
        
        # è¾“å‡ºå½’ä¸€åŒ–
        x = self.output_ln(x)
        
        return x


# --- ç¼–ç å™¨ï¼ˆé‡å‘½åä¸ºencoderï¼‰---
class Encoder(nn.Module):
    """ç¼–ç å™¨ï¼Œè¾“å…¥210*160*3å›¾åƒ"""

    def __init__(self, in_channels: int = 3, layers: List[int] = [2, 3, 4, 3], base_channels: int = 64) -> None:
        super().__init__()

        # Stemå±‚ - å¤„ç†210*160*3è¾“å…¥
        self.stem = nn.Sequential(
            nn.Conv2d(in_channels, base_channels // 2, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(base_channels // 2),
            nn.ReLU(inplace=True),
            nn.Conv2d(base_channels // 2, base_channels, kernel_size=3, stride=2, padding=1, bias=False),  # 105*80
            nn.BatchNorm2d(base_channels),
            nn.ReLU(inplace=True),
        )

        # ç‰¹å¾æå–ç½‘ç»œ
        self.layer1 = DownBlock2d(base_channels, base_channels, layers[0], downsample=False)       # 105*80
        self.layer2 = DownBlock2d(base_channels, base_channels * 2, layers[1], downsample=True)    # 53*40
        self.layer3 = DownBlock2d(base_channels * 2, base_channels * 4, layers[2], downsample=True) # 27*20
        self.layer4 = DownBlock2d(base_channels * 4, base_channels * 8, layers[3], downsample=True) # 14*10

        # å…¨å±€å¹³å‡æ± åŒ–
        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))

        # ç‰¹å¾æŠ•å½±
        self.feature_proj = nn.Sequential(
            nn.Linear(base_channels * 8, 512),
            nn.LayerNorm(512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.LayerNorm(256)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: è¾“å…¥å›¾åƒ (B, 3, 210, 160)
        Returns:
            features: ç‰¹å¾å‘é‡ (B, 256)
        """
        # ç¡®ä¿è¾“å…¥å°ºå¯¸æ­£ç¡®
        assert x.shape[-2:] == (210, 160), f"Expected input size (210, 160), got {x.shape[-2:]}"

        x = self.stem(x)  # (B, 64, 105, 80)
        x = self.layer1(x)  # (B, 64, 105, 80)
        x = self.layer2(x)  # (B, 128, 53, 40)
        x = self.layer3(x)  # (B, 256, 27, 20)
        x = self.layer4(x)  # (B, 512, 14, 10)

        # å…¨å±€æ± åŒ–å’Œç‰¹å¾æŠ•å½±
        pooled = self.global_pool(x)  # (B, 512, 1, 1)
        flattened = pooled.view(pooled.size(0), -1)  # (B, 512)
        features = self.feature_proj(flattened)  # (B, 256)

        return features


# --- PRM_JEä¸»æ¨¡å‹ ---
class PRM_JE(nn.Module):
    """Predictive Representation Model - Just Encoder (åªåŒ…å«ç¼–ç å™¨å’Œé¢„æµ‹æ¨¡å‹)"""
    
    def __init__(
        self, 
        img_in_channels: int = 3,
        encoder_layers: List[int] = [2, 3, 4, 3],
        action_dim: int = 19,  # 0-18å…±19ä¸ªåŠ¨ä½œ
        latent_dim: int = 256,
        base_channels: int = 64, 
        num_attention_heads: int = 8,
        transformer_layers: int = 3,
        loss_weight: float = 1.0,  # åªæœ‰ä¸€ä¸ªæŸå¤±æƒé‡
        dropout: float = 0.1
    ) -> None:
        super().__init__()
        
        # ç¼–ç å™¨
        self.encoder = Encoder(img_in_channels, encoder_layers, base_channels)
        
        # ä¸‰å±‚Transformeré¢„æµ‹æ¨¡å‹
        self.predictive_model = PredictiveModel(
            d_model=latent_dim,
            n_heads=num_attention_heads,
            n_layers=transformer_layers,
            action_dim=action_dim,
            dropout=dropout
        )
        
        self.loss_weight = loss_weight
        
        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)
        
    def _init_weights(self, m):
        """å‚è€ƒLLMçš„æƒé‡åˆå§‹åŒ–ç­–ç•¥"""
        if isinstance(m, nn.Linear):
            # Xavier uniform for linear layers
            nn.init.xavier_uniform_(m.weight)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.Embedding):
            # æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–embedding
            nn.init.normal_(m.weight, meas zn=0, std=0.02)
        elif isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
            # Kaimingåˆå§‹åŒ–å·ç§¯å±‚
            nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, (nn.BatchNorm2d, nn.LayerNorm)):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
    
    def forward(self, images: torch.Tensor, actions: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        Args:
            images: è¾“å…¥å›¾åƒ (B, 3, 210, 160)
            actions: åŠ¨ä½œ (B,) - 0åˆ°18çš„æ•´æ•°
        Returns:
            encoder_features: ç¼–ç å™¨è¾“å‡º (B, 256)
            predicted_features: é¢„æµ‹æ¨¡å‹è¾“å‡º (B, 256)
        """
        # ç¡®ä¿åŠ¨ä½œåœ¨æ­£ç¡®èŒƒå›´å†…
        assert actions.min() >= 0 and actions.max() <= 18, f"Actions must be in range [0, 18], got [{actions.min()}, {actions.max()}]"
        
        # ç¼–ç 
        encoder_features = self.encoder(images)  # (B, 256)
        # Transformeré¢„æµ‹
        predicted_features = self.predictive_model(encoder_features, actions)  # (B, 256)
        
        return encoder_features, predicted_features
    
    def compute_loss(self, 
                     predicted_features: torch.Tensor,
                     target_features: torch.Tensor) -> Tuple[torch.Tensor, dict]:
        """
        è®¡ç®—ç‰¹å¾é¢„æµ‹æŸå¤±
        Args:
            predicted_features: é¢„æµ‹ç‰¹å¾ (B, 256)
            target_features: ç›®æ ‡ç‰¹å¾ (B, 256)
        """
        # Loss_vector: ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤± - æ‰¹æ¬¡å†…å¹³å‡
        pred_norm = F.normalize(predicted_features, p=2, dim=1)
        target_norm = F.normalize(target_features, p=2, dim=1)
        cosine_sim = (pred_norm * target_norm).sum(dim=1)
        loss_vector = (1.0 - cosine_sim).mean()  # ä½™å¼¦è·ç¦»çš„å¹³å‡
        
        # æ€»æŸå¤±
        total_loss = self.loss_weight * loss_vector
        
        loss_dict = {
            'total_loss': total_loss.item(),
            'loss_vector': loss_vector.item(),
            'cosine_similarity': cosine_sim.mean().item()
        }
        
        return total_loss, loss_dict

    def train_on_batch(self, 
                      batch_images: torch.Tensor,
                      batch_actions: torch.Tensor,
                      batch_next_images: torch.Tensor,
                      optimizer: torch.optim.Optimizer) -> dict:
        """
        åœ¨å•ä¸ªbatchä¸Šè®­ç»ƒ
        Args:
            batch_images: å½“å‰å›¾åƒ [I1,I2,I3...] (B, 3, 210, 160)
            batch_actions: åŠ¨ä½œ (B,)
            batch_next_images: ä¸‹ä¸€å¸§å›¾åƒ (B, 3, 210, 160) 
            optimizer: ä¼˜åŒ–å™¨
        """
        self.train()
        optimizer.zero_grad()
        
        batch_size = batch_images.size(0)
        if batch_size <= 1:
            raise ValueError("Batch size must be greater than 1")
        
        # æŒ‰ç…§è¦æ±‚ï¼šå¯¹batchä¸­å‰mä¸ªæ ·æœ¬è¿›è¡Œå¤„ç† (1<=m<=batch_size-1)
        # ä½¿ç”¨I1,I2,...,Imé¢„æµ‹I2',I3',...,I(m+1)'çš„ç‰¹å¾
        input_images = batch_images[:-1]    # [I1,I2,...,Im]
        input_actions = batch_actions[:-1]  # å¯¹åº”çš„åŠ¨ä½œ
        target_next_images = batch_next_images[:-1]  # [I2,I3,...,I(m+1)] - çœŸå®çš„ä¸‹ä¸€å¸§
        
        # å‰å‘ä¼ æ’­ï¼šç”¨[I1,I2,...,Im]å’Œå¯¹åº”åŠ¨ä½œé¢„æµ‹ä¸‹ä¸€å¸§ç‰¹å¾
        encoder_features, predicted_features = self.forward(input_images, input_actions)
        
        # è·å–ç›®æ ‡encoderç‰¹å¾ - å¯¹çœŸå®çš„ä¸‹ä¸€å¸§å›¾åƒç¼–ç 
        with torch.no_grad():
            target_encoder_features = self.encoder(target_next_images)  # [x2,x3,...,x(m+1)]
        
        # è®¡ç®—æŸå¤±
        total_loss, loss_dict = self.compute_loss(predicted_features, target_encoder_features)
        
        # åå‘ä¼ æ’­å’Œä¼˜åŒ–
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)
        optimizer.step()
        
        return loss_dict

    def evaluate_on_batch(self,
                         batch_images: torch.Tensor,
                         batch_actions: torch.Tensor, 
                         batch_next_images: torch.Tensor) -> dict:
        """
        åœ¨å•ä¸ªbatchä¸ŠéªŒè¯
        Args:
            batch_images: å½“å‰å›¾åƒ [I1,I2,I3...] (B, 3, 210, 160)
            batch_actions: åŠ¨ä½œ (B,)
            batch_next_images: ä¸‹ä¸€å¸§å›¾åƒ (B, 3, 210, 160)
        Returns:
            loss_dict: åŒ…å«å„é¡¹æŸå¤±çš„å­—å…¸
        """
        self.eval()
        
        batch_size = batch_images.size(0)
        if batch_size <= 1:
            raise ValueError("Batch size must be greater than 1")
        
        with torch.no_grad():
            # æŒ‰ç…§è¦æ±‚ï¼šå¯¹batchä¸­å‰mä¸ªæ ·æœ¬è¿›è¡Œå¤„ç† (1<=m<=batch_size-1)
            input_images = batch_images[:-1]    # [I1,I2,...,Im]
            input_actions = batch_actions[:-1]  # å¯¹åº”çš„åŠ¨ä½œ
            target_next_images = batch_next_images[:-1]  # [I2,I3,...,I(m+1)]
            
            # å‰å‘ä¼ æ’­
            encoder_features, predicted_features = self.forward(input_images, input_actions)
            
            # è·å–ç›®æ ‡encoderç‰¹å¾
            target_encoder_features = self.encoder(target_next_images)  # [x2,x3,...,x(m+1)]
            
            # è®¡ç®—æŸå¤±
            total_loss, loss_dict = self.compute_loss(predicted_features, target_encoder_features)
            
        return loss_dict


def create_optimized_optimizer(model, base_lr=1e-4, weight_decay=1e-2):
    """
    åˆ›å»ºä¼˜åŒ–çš„ä¼˜åŒ–å™¨é…ç½®ï¼Œå‚è€ƒLLMè®­ç»ƒæœ€ä½³å®è·µ
    """
    # åˆ†ç¦»ä¸åŒç±»å‹çš„å‚æ•°
    decay_params = []
    no_decay_params = []
    
    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue
        # LayerNorm, BatchNorm, biasç­‰ä¸è¿›è¡Œæƒé‡è¡°å‡
        if len(param.shape) <= 1 or name.endswith(".bias") or "norm" in name.lower():
            no_decay_params.append(param)
        else:
            decay_params.append(param)
    
    optimizer_grouped_parameters = [
        {
            "params": decay_params,
            "weight_decay": weight_decay,
            "lr": base_lr
        },
        {
            "params": no_decay_params,
            "weight_decay": 0.0,
            "lr": base_lr
        }
    ]
    
    # ä½¿ç”¨AdamWä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        optimizer_grouped_parameters,
        lr=base_lr,
        betas=(0.9, 0.95),  # å‚è€ƒLLMè®­ç»ƒçš„betaå€¼
        eps=1e-8,
        weight_decay=weight_decay
    )
    
    return optimizer


def create_lr_scheduler(optimizer, warmup_steps=1000, max_steps=10000, min_lr_ratio=0.1):
    """
    åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ŒåŒ…å«warmupå’Œcosine annealing
    """
    def lr_lambda(step):
        if step < warmup_steps:
            # Warmupé˜¶æ®µ
            return step / warmup_steps
        else:
            # Cosine annealingé˜¶æ®µ
            progress = (step - warmup_steps) / (max_steps - warmup_steps)
            progress = min(progress, 1.0)
            cosine_decay = 0.5 * (1 + math.cos(math.pi * progress))
            return min_lr_ratio + (1 - min_lr_ratio) * cosine_decay
    
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    return scheduler


if __name__ == '__main__':
    # --- æ¨¡å‹æµ‹è¯• ---
    BATCH_SIZE = 8
    IMG_C = 3
    IMG_H = 210
    IMG_W = 160
    ACTION_DIM = 19  # 0-18å…±19ä¸ªåŠ¨ä½œ

    print("=== PRM_JE Model Testing ===")
    print(f"Input image size: {IMG_H}x{IMG_W}x{IMG_C}")
    print(f"Action range: 0-{ACTION_DIM-1}")
    print(f"Batch size: {BATCH_SIZE}")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    dummy_images = torch.rand(BATCH_SIZE, IMG_C, IMG_H, IMG_W)  # [0,1]èŒƒå›´
    dummy_actions = torch.randint(0, ACTION_DIM, (BATCH_SIZE,))  # 0-18çš„åŠ¨ä½œ
    dummy_next_images = torch.rand(BATCH_SIZE, IMG_C, IMG_H, IMG_W)  # [0,1]èŒƒå›´

    # åˆ›å»ºæ¨¡å‹
    model = PRM_JE(
        img_in_channels=IMG_C,
        encoder_layers=[2, 3, 4, 3],
        action_dim=ACTION_DIM,
        latent_dim=256,
        base_channels=64,
        num_attention_heads=8,
        transformer_layers=3,
        loss_weight=1.0,
        dropout=0.1
    )

    # å‚æ•°ç»Ÿè®¡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\næ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")

    # æµ‹è¯•å‰å‘ä¼ æ’­
    print(f"\n=== å‰å‘ä¼ æ’­æµ‹è¯• ===")
    model.eval()
    with torch.no_grad():
        encoder_features, predicted_features = model(dummy_images, dummy_actions)
        print(f"âœ… ç¼–ç å™¨ç‰¹å¾: {encoder_features.shape}")
        print(f"âœ… é¢„æµ‹ç‰¹å¾: {predicted_features.shape}")

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = create_optimized_optimizer(model, base_lr=1e-4, weight_decay=1e-2)
    scheduler = create_lr_scheduler(optimizer, warmup_steps=100, max_steps=1000)

    # æµ‹è¯•è®­ç»ƒ
    print(f"\n=== è®­ç»ƒæµ‹è¯• ===")
    n_epochs = 50
    
    for epoch in range(n_epochs):
        # è®­ç»ƒ
        loss_dict = model.train_on_batch(
            dummy_images, dummy_actions, dummy_next_images, optimizer
        )
        scheduler.step()
        
        if (epoch + 1) % 10 == 0:
            current_lr = optimizer.param_groups[0]['lr']
            print(f'Epoch {epoch + 1:3d}/{n_epochs}, '
                  f'Total: {loss_dict["total_loss"]:.6f}, '
                  f'Cosine Sim: {loss_dict["cosine_similarity"]:.4f}, '
                  f'LR: {current_lr:.2e}')
            
            # éªŒè¯
            val_loss_dict = model.evaluate_on_batch(
                dummy_images, dummy_actions, dummy_next_images
            )
            print(f'    Validation: {val_loss_dict["total_loss"]:.6f}')

    print(f"\n=== PRM_JEæ¨¡å‹æ€»ç»“ ===")
    print(f"âœ… ç¼–ç å™¨: å¤„ç†210Ã—160Ã—3å›¾åƒ â†’ 256ç»´ç‰¹å¾")
    print(f"âœ… åŠ¨ä½œç¼–ç : 0-18æ•´æ•°åŠ¨ä½œembedding")
    print(f"âœ… æ³¨æ„åŠ›: åŠ¨ä½œä½œä¸ºqueryï¼Œè§†è§‰ç‰¹å¾ä½œä¸ºkey/value")
    print(f"âœ… é¢„æµ‹æ¨¡å‹: è¾“å‡ºä¸‹ä¸€å¸§çš„ç‰¹å¾è¡¨ç¤º")
    print(f"âœ… æŸå¤±å‡½æ•°: ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±")
    print(f"âœ… ä¼˜åŒ–: AdamW + warmup + ä½™å¼¦é€€ç«")
    print(f"âœ… æ€»å‚æ•°: {total_params:,}")
    print("ğŸ‰ PRM_JEæ¨¡å‹æµ‹è¯•å®Œæˆï¼")
