# main_runner.py
import torch
import torch.optim as optim
import numpy as np
import time # ç”¨äºè®¡æ—¶

# ä»å…¶ä»–æ¨¡å—å¯¼å…¥
from env.AtariEnv import AtariEnvManager
from model.PPM import PredictiveRepModel
import wandb

# --- ğŸ”´ é…ç½®å‚æ•° ---
GAME_NAME = "Seaquest"          # Atari æ¸¸æˆåç§° ("RiverRaid-v4", "ChopperCommand" ç­‰)
NUM_ENVS = 4                    # å¹¶è¡Œç¯å¢ƒæ•°é‡ (ç”¨äºæ•°æ®æ”¶é›†)
BATCH_SIZE = 32                 # æ¨¡å‹è®­ç»ƒçš„æ‰¹æ¬¡å¤§å°
LEARNING_RATE = 1e-4            # å­¦ä¹ ç‡
NUM_TRAINING_STEPS = 1000      # æ€»è®­ç»ƒæ­¥æ•°
EVAL_INTERVAL = 500             # æ¯éš”å¤šå°‘æ­¥è¿›è¡Œä¸€æ¬¡è¯„ä¼°
PRINT_INTERVAL = 20             # æ¯éš”å¤šå°‘æ­¥æ‰“å°ä¸€æ¬¡è®­ç»ƒæŸå¤±
SAVE_INTERVAL = 500            # æ¯éš”å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {DEVICE}")

# Atari è§‚æµ‹å±æ€§ (æ¥è‡ªç¯å¢ƒ)
# è¿™äº›å€¼åœ¨é¢„å¤„ç†å’Œæ¨¡å‹åˆå§‹åŒ–æ—¶ä½¿ç”¨
RAW_IMG_HEIGHT = 210
RAW_IMG_WIDTH = 160
RAW_IMG_CHANNELS = 3

# æ¨¡å‹å‚æ•°
MODEL_ENCODER_LAYERS = [2, 2]   # ç¼–ç å™¨ä¸­æ¯ä¸ª DownBlock çš„å±‚æ•°
# è§£ç å™¨æœ‰3ä¸ªUpBlock, è¿™é‡Œé…ç½®æ¯ä¸ªUpBlockçš„å±‚æ•°
MODEL_DECODER_LAYERS_CONFIG = [MODEL_ENCODER_LAYERS[1], MODEL_ENCODER_LAYERS[0], 1] # ä¾‹å¦‚ [2,2,1]
MODEL_OUTPUT_CHANNELS = RAW_IMG_CHANNELS # æ¨¡å‹è¾“å‡ºé€šé“æ•° (é€šå¸¸ä¸è¾“å…¥ä¸€è‡´)

# --- æ•°æ®é¢„å¤„ç†å‡½æ•° ---
def preprocess_observations_actions(obs_batch_np, actions_batch_np, device):
    """
    å°† NumPy æ ¼å¼çš„è§‚æµ‹å’ŒåŠ¨ä½œæ‰¹æ¬¡è½¬æ¢ä¸º PyTorch å¼ é‡ï¼Œå¹¶è¿›è¡Œé¢„å¤„ç†ã€‚
    å‚æ•°:
        obs_batch_np (np.ndarray): å½¢çŠ¶ (N, H, W, C) çš„è§‚æµ‹æ•°æ®ã€‚
        actions_batch_np (np.ndarray): å½¢çŠ¶ (N,) çš„åŠ¨ä½œæ•°æ®ã€‚
        device: PyTorch è®¾å¤‡ã€‚
    è¿”å›:
        obs_tensor (torch.Tensor): å½¢çŠ¶ (N, 1, C, H, W)
        actions_tensor (torch.Tensor): å½¢çŠ¶ (N, 1)
    """
    # 1. å¤„ç†è§‚æµ‹æ•°æ®
    #    ä» (N, H, W, C) è½¬ä¸º (N, C, H, W)ï¼Œå½’ä¸€åŒ–åˆ° [0, 1]
    obs_tensor_n_c_h_w = torch.tensor(obs_batch_np, dtype=torch.float32, device=device)
    obs_tensor_n_c_h_w = obs_tensor_n_c_h_w.permute(0, 3, 1, 2) / 255.0
    
    #    å¢åŠ åºåˆ—é•¿åº¦ç»´åº¦ L=1: (N, C, H, W) -> (N, 1, C, H, W)
    obs_tensor_n_l_c_h_w = obs_tensor_n_c_h_w.unsqueeze(1)

    # 2. å¤„ç†åŠ¨ä½œæ•°æ®
    actions_tensor_n = torch.tensor(actions_batch_np, dtype=torch.long, device=device)
    #    å¢åŠ åºåˆ—é•¿åº¦ç»´åº¦ L=1: (N,) -> (N, 1)
    actions_tensor_n_l = actions_tensor_n.unsqueeze(1)
    
    return obs_tensor_n_l_c_h_w, actions_tensor_n_l

# --- ä¸»è®­ç»ƒä¸è¯„ä¼°æµç¨‹ ---
def main():
    print("--- åˆå§‹åŒ–ç¯å¢ƒå’Œæ¨¡å‹ ---")
    # 1. åˆå§‹åŒ–ç¯å¢ƒç®¡ç†å™¨
    #    è®­ç»ƒæ—¶é€šå¸¸ä¸è¿›è¡Œæ¸²æŸ“ (render_mode=None)
    env_manager = AtariEnvManager(GAME_NAME, num_envs=NUM_ENVS, render_mode=None)

    # 2. åˆå§‹åŒ–æ¨¡å‹
    model = PredictiveRepModel(
        img_in_channels=RAW_IMG_CHANNELS,
        img_out_channels=MODEL_OUTPUT_CHANNELS,
        encoder_layers=MODEL_ENCODER_LAYERS,
        decoder_layers_config=MODEL_DECODER_LAYERS_CONFIG,
        target_img_h=RAW_IMG_HEIGHT, # ç›®æ ‡å›¾åƒé«˜åº¦ (ç”¨äºè£å‰ªæ¨¡å‹è¾“å‡º)
        target_img_w=RAW_IMG_WIDTH,  # ç›®æ ‡å›¾åƒå®½åº¦
    ).to(DEVICE)

    # 3. åˆå§‹åŒ–ä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    print(f"--- å¼€å§‹åœ¨ {DEVICE} ä¸Šè®­ç»ƒ {NUM_TRAINING_STEPS} æ­¥ ---")
    start_time = time.time()

    # è®­ç»ƒå¾ªç¯
    for step in range(1, NUM_TRAINING_STEPS + 1):
        # --- æ”¶é›†ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ® ---
        batch_obs1_list, batch_actions_list, batch_obs2_list = [], [], []
        
        # å¾ªç¯ç›´åˆ°æ”¶é›†åˆ°è¶³å¤Ÿçš„ BATCH_SIZE ä¸ªç»éªŒ
        while len(batch_obs1_list) < BATCH_SIZE:
            # ä»æ‰€æœ‰å¹¶è¡Œç¯å¢ƒä¸­é‡‡æ ·åŠ¨ä½œ
            actions_to_take_in_envs = env_manager.sample_actions()
            
            # åœ¨æ‰€æœ‰ç¯å¢ƒä¸­æ‰§è¡Œä¸€æ­¥ï¼Œè·å–ç»éªŒå…ƒç»„åˆ—è¡¨
            # experiences_from_envs: [(obs1, action, obs2, reward, done), ...]
            experiences_from_envs = env_manager.step(actions_to_take_in_envs)
            
            for obs1_np, action_int, obs2_np, _, _ in experiences_from_envs:
                if len(batch_obs1_list) < BATCH_SIZE: # ç¡®ä¿ä¸è¶…è¿‡ BATCH_SIZE
                    batch_obs1_list.append(obs1_np)       # HWC, uint8
                    batch_actions_list.append(action_int) # int
                    batch_obs2_list.append(obs2_np)       # HWC, uint8
                else:
                    break # å½“å‰æ‰¹æ¬¡æ•°æ®å·²æ»¡

        # å°†åˆ—è¡¨è½¬æ¢ä¸º NumPy æ•°ç»„
        obs1_np_batch = np.array(batch_obs1_list)
        actions_np_batch = np.array(batch_actions_list)
        obs2_np_batch = np.array(batch_obs2_list)

        # --- æ•°æ®é¢„å¤„ç† ---
        # obs1_input: (BATCH_SIZE, 1, C, H, W), float, normalized
        # actions_input: (BATCH_SIZE, 1), long
        obs1_input, actions_input = preprocess_observations_actions(obs1_np_batch, actions_np_batch, DEVICE)
        
        # obs2_target ä¹Ÿéœ€è¦åŒæ ·çš„å¤„ç† (é™¤äº†åŠ¨ä½œ)
        obs2_target, _ = preprocess_observations_actions(obs2_np_batch, np.zeros_like(actions_np_batch), DEVICE) # åŠ¨ä½œéƒ¨åˆ†ä¸ä½¿ç”¨

        # --- æ¨¡å‹è®­ç»ƒ ---
        train_loss = model.train_on_batch(obs1_input, actions_input, obs2_target, optimizer)

        # --- æ‰“å°è®­ç»ƒä¿¡æ¯ ---
        if step % PRINT_INTERVAL == 0:
            elapsed_time = time.time() - start_time
            print(f"è®­ç»ƒæ­¥ [{step}/{NUM_TRAINING_STEPS}], "
                  f"æŸå¤±: {train_loss:.6f}, "
                  f"è€—æ—¶: {elapsed_time:.2f}s")
            start_time = time.time() # é‡ç½®è®¡æ—¶å™¨

        # --- å®šæœŸè¯„ä¼° ---
        if step % EVAL_INTERVAL == 0:
            # æ”¶é›†è¯„ä¼°æ•°æ® (ä¸è®­ç»ƒæ•°æ®æ”¶é›†æ–¹å¼ç›¸åŒï¼Œä½†é€šå¸¸æ¥è‡ªä¸åŒçš„æ•°æ®é›†æˆ–çŠ¶æ€)
            # è¿™é‡Œä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç±»ä¼¼çš„æ–¹å¼æ”¶é›†æ–°çš„ä¸€æ‰¹æ•°æ®è¿›è¡Œè¯„ä¼°
            eval_obs1_list, eval_actions_list, eval_obs2_list = [], [], []
            while len(eval_obs1_list) < BATCH_SIZE: # ä½¿ç”¨ç›¸åŒçš„ BATCH_SIZE è¿›è¡Œè¯„ä¼°
                actions_to_take_in_envs = env_manager.sample_actions()
                experiences_from_envs = env_manager.step(actions_to_take_in_envs)
                for o1, a, o2, _, _ in experiences_from_envs:
                    if len(eval_obs1_list) < BATCH_SIZE:
                        eval_obs1_list.append(o1)
                        eval_actions_list.append(a)
                        eval_obs2_list.append(o2)
                    else: break
            
            eval_o1_np = np.array(eval_obs1_list)
            eval_a_np = np.array(eval_actions_list)
            eval_o2_np = np.array(eval_obs2_list)

            eval_obs1_input, eval_actions_input = preprocess_observations_actions(eval_o1_np, eval_a_np, DEVICE)
            eval_obs2_target, _ = preprocess_observations_actions(eval_o2_np, np.zeros_like(eval_a_np), DEVICE)
            
            eval_loss = model.evaluate_on_batch(eval_obs1_input, eval_actions_input, eval_obs2_target)
            print(f"--- è¯„ä¼°æ­¥ [{step}/{NUM_TRAINING_STEPS}], è¯„ä¼°æŸå¤±: {eval_loss:.6f} ---")
        
        # --- å®šæœŸä¿å­˜æ¨¡å‹ ---
        if step % SAVE_INTERVAL == 0:
            model_save_path = f"{GAME_NAME.lower()}_predcoder_step_{step}.pth"
            torch.save(model.state_dict(), model_save_path)
            print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_save_path}")

    print("--- è®­ç»ƒå®Œæˆ ---")
    final_model_path = f"{GAME_NAME.lower()}_predcoder_final.pth"
    torch.save(model.state_dict(), final_model_path)
    print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}")

    env_manager.close() # å…³é—­ç¯å¢ƒ
    # --- Wandb åˆå§‹åŒ–å’Œé…ç½® ---

    # åˆå§‹åŒ– wandb
    wandb.init(
        project=f"predcoder-{GAME_NAME}",
        config={
            "game": GAME_NAME,
            "num_envs": NUM_ENVS,
            "batch_size": BATCH_SIZE,
            "learning_rate": LEARNING_RATE,
            "num_training_steps": NUM_TRAINING_STEPS,
            "encoder_layers": MODEL_ENCODER_LAYERS,
            "decoder_layers": MODEL_DECODER_LAYERS_CONFIG,
        }
    )

    def log_metrics(step, train_loss=None, eval_loss=None):
        """è®°å½•è®­ç»ƒæŒ‡æ ‡åˆ° wandb"""
        metrics = {}
        if train_loss is not None:
            metrics["train_loss"] = train_loss
        if eval_loss is not None:
            metrics["eval_loss"] = eval_loss
        wandb.log(metrics, step=step)

    # åœ¨ä¸»å‡½æ•°ç»“æŸæ—¶å…³é—­ wandb
    def cleanup():
        """æ¸…ç†å‡½æ•°"""
        wandb.finish()

if __name__ == '__main__':
    main()