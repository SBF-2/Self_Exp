"""
è¿‡æ‹Ÿåˆè®­ç»ƒè„šæœ¬ - éªŒè¯æ¨¡å‹å’Œä»£ç æ­£ç¡®æ€§
=============================================

ç›®æ ‡ï¼š
1. æ”¶é›†128æ¡è®­ç»ƒæ•°æ®ï¼ˆä½¿ç”¨åŸå§‹3ä¸ªAtariç¯å¢ƒï¼‰
2. ç”¨è¿™128æ¡æ•°æ®å¾ªç¯è®­ç»ƒæ¨¡å‹ç›´åˆ°æŸå¤±<20
3. è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜æ¯ä¸ªç¯å¢ƒä¸‹æŸå¤±æœ€å°çš„4å¼ å¯¹ç…§å›¾
4. ä¿å­˜è¿‡æ‹Ÿåˆæ¨¡å‹ï¼ŒéªŒè¯è®­ç»ƒæµç¨‹æ­£ç¡®æ€§
5. æ‰€æœ‰æ–‡ä»¶ååŠ overfitå‰ç¼€ï¼Œä¸è¦†ç›–åŸæœ‰æ–‡ä»¶
"""

import torch
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import os
import pickle
from datetime import datetime
from collections import defaultdict

# Import your custom modules
from model.PPM_attention import PredictiveRepModel
from env.AtariEnv_random import AtariEnvManager

# --- Configuration ---
# Environment parameters (ä½¿ç”¨åŸå§‹3ä¸ªç¯å¢ƒ)
NUM_GAMES_TO_SELECT_FROM = 3  # åŸå§‹3ä¸ªæ¸¸æˆç¯å¢ƒ
NUM_ENVS = 128                # 128ä¸ªå¹¶è¡Œç¯å¢ƒæ”¶é›†æ•°æ®
FIXED_DATA_SIZE = 128         # å›ºå®šä½¿ç”¨128æ¡æ•°æ®
IMG_H, IMG_W, IMG_C = 210, 160, 3

# Model parameters
LATENT_DIM = 256
ENCODER_LAYERS = [2, 2]
DECODER_LAYERS = [2, 2, 1]

# Training parameters
LEARNING_RATE = 1e-3          # ç¨é«˜çš„å­¦ä¹ ç‡åŠ é€Ÿè¿‡æ‹Ÿåˆ
TARGET_LOSS = 20.0            # ç›®æ ‡æŸå¤±é˜ˆå€¼
MAX_EPOCHS = 40000             # æœ€å¤§è®­ç»ƒè½®æ•°
PRINT_INTERVAL = 200         # æ‰“å°é—´éš”
SAVE_INTERVAL = 5000           # ä¿å­˜é—´éš”
EVAL_INTERVAL = 500          # è¯„ä¼°å¹¶ä¿å­˜å¯¹æ¯”å›¾é—´éš”
SEQ_LEN = 1

# Overfit experiment paths (æ‰€æœ‰æ–‡ä»¶ååŠ overfitå‰ç¼€)
OVERFIT_MODEL_DIR = "train_models_attention/overfit_trained_models"
OVERFIT_DATA_FILE = "overfit_training_data_128.pkl"
OVERFIT_LOG_FILE = "log/overfit_training.log"
OVERFIT_IMAGES_DIR = "overfit_training_images"  # è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¯¹æ¯”å›¾
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def preprocess_observations(obs_list, device):
    """
    Converts a list of observations (H, W, C) uint8 to a tensor (B, C, H, W) float.
    """
    processed_obs = []
    for obs_hwc in obs_list:
        obs_chw = np.transpose(obs_hwc, (2, 0, 1)) # HWC to CHW
        processed_obs.append(obs_chw)
    
    # Stack, normalize to [0, 1], convert to tensor
    obs_tensor_bchw = torch.tensor(np.array(processed_obs), dtype=torch.float32, device=device)
    return obs_tensor_bchw

def denormalize_image(img_tensor):
    """Convert tensor back to displayable format"""
    # Clamp values to [0, 255] range and convert to uint8
    img_np = torch.clamp(img_tensor, 0, 255).cpu().numpy().astype(np.uint8)
    return np.transpose(img_np, (1, 2, 0))  # CHW to HWC

def create_comparison_image(real_img, pred_img, sample_idx, loss_value, env_name, epoch):
    """Create a side-by-side comparison image during training"""
    fig, axes = plt.subplots(1, 2, figsize=(12, 6))
    
    # Real image (left)
    axes[0].imshow(real_img)
    axes[0].set_title('Real Image', fontsize=14, fontweight='bold')
    axes[0].axis('off')
    
    # Predicted image (right)
    axes[1].imshow(pred_img)
    axes[1].set_title('Generated Image', fontsize=14, fontweight='bold')
    axes[1].axis('off')
    
    # Overall title
    fig.suptitle(f'{env_name} - Training Epoch {epoch}\nSample {sample_idx} - MSE Loss: {loss_value:.6f}', 
                 fontsize=16, fontweight='bold')
    
    plt.tight_layout()
    return fig

def classify_samples_by_environment(dataset):
    """
    æ ¹æ®å›¾åƒç‰¹å¾æˆ–å…¶ä»–ä¿¡æ¯å°†æ ·æœ¬åˆ†ç±»åˆ°ä¸åŒç¯å¢ƒ
    ç”±äºæˆ‘ä»¬æ²¡æœ‰æ˜¾å¼çš„ç¯å¢ƒæ ‡ç­¾ï¼Œè¿™é‡Œä½¿ç”¨ç®€å•çš„å“ˆå¸Œæ–¹æ³•æ¥æ¨¡æ‹Ÿç¯å¢ƒåˆ†ç±»
    """
    env_samples = defaultdict(list)
    
    for idx, data in enumerate(dataset):
        # ä½¿ç”¨obs1çš„å“ˆå¸Œå€¼æ¥æ¨¡æ‹Ÿç¯å¢ƒåˆ†ç±»
        # åœ¨çœŸå®æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥æ ¹æ®æ¸¸æˆç”»é¢ç‰¹å¾æˆ–å…¶ä»–ä¿¡æ¯æ¥åˆ†ç±»
        obs_hash = hash(data['obs1'].tobytes()) % 3  # åˆ†æˆ3ä¸ªç¯å¢ƒ
        env_name = ['Seaquest-v4', 'Riverraid-v4', 'ChopperCommand-v4'][obs_hash]
        env_samples[env_name].append(idx)
    
    return env_samples

def evaluate_and_save_best_samples(model, dataset, env_samples, epoch, images_dir):
    """
    è¯„ä¼°æ¨¡å‹å¹¶ä¿å­˜æ¯ä¸ªç¯å¢ƒä¸‹æŸå¤±æœ€å°çš„4å¼ å¯¹ç…§å›¾
    """
    model.eval()
    
    # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
    obs1_input, actions_input, obs2_target = create_batch_from_dataset(dataset, DEVICE)
    
    with torch.no_grad():
        # æ¨¡å‹æ¨ç†
        _, predicted_obs2_raw = model(obs1_input, actions_input)
        
        # å¤„ç†è¾“å‡º
        predicted_obs2_processed = predicted_obs2_raw.squeeze(1)  # (B, C, H, W)
        obs2_target_processed = obs2_target.squeeze(1)  # (B, C, H, W)
        
        # è£å‰ªé¢„æµ‹ç»“æœ
        predicted_obs2_cropped = model._crop_output(predicted_obs2_processed)
        
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŸå¤±
        individual_losses = []
        for i in range(predicted_obs2_cropped.shape[0]):
            loss_i = F.mse_loss(
                predicted_obs2_cropped[i:i+1], 
                obs2_target_processed[i:i+1]
            ).item()
            individual_losses.append(loss_i)
    
    # ä¸ºæ¯ä¸ªç¯å¢ƒä¿å­˜æœ€ä½³4å¼ å¯¹æ¯”å›¾
    total_saved = 0
    for env_name, sample_indices in env_samples.items():
        if len(sample_indices) == 0:
            continue
        
        # è·å–è¯¥ç¯å¢ƒä¸‹æ‰€æœ‰æ ·æœ¬çš„æŸå¤±
        env_losses = [(idx, individual_losses[idx]) for idx in sample_indices]
        env_losses.sort(key=lambda x: x[1])  # æŒ‰æŸå¤±æ’åº
        
        # å–å‰4ä¸ªæœ€ä½³æ ·æœ¬
        best_samples = env_losses[:min(4, len(env_losses))]
        
        # åˆ›å»ºç¯å¢ƒç‰¹å®šçš„ç›®å½•
        env_dir = os.path.join(images_dir, f"epoch_{epoch}", env_name.replace('-v4', ''))
        os.makedirs(env_dir, exist_ok=True)
        
        # ä¿å­˜æœ€ä½³æ ·æœ¬çš„å¯¹æ¯”å›¾
        for rank, (sample_idx, loss_val) in enumerate(best_samples):
            real_img = dataset[sample_idx]['obs2']
            pred_img = denormalize_image(predicted_obs2_cropped[sample_idx])
            
            fig = create_comparison_image(
                real_img, pred_img, sample_idx + 1, loss_val, env_name, epoch
            )
            
            save_path = os.path.join(env_dir, f'best_{rank+1}_sample_{sample_idx+1}_loss_{loss_val:.6f}.png')
            fig.savefig(save_path, dpi=150, bbox_inches='tight')
            plt.close(fig)
            total_saved += 1
        
        print(f"    {env_name}: ä¿å­˜äº† {len(best_samples)} å¼ æœ€ä½³å¯¹æ¯”å›¾")
    
    model.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼
    return np.mean(individual_losses), total_saved

def collect_fixed_dataset(env_manager, target_size=128):
    """æ”¶é›†å›ºå®šå¤§å°çš„æ•°æ®é›†"""
    print(f"å¼€å§‹æ”¶é›† {target_size} æ¡å›ºå®šè®­ç»ƒæ•°æ®...")
    
    collected_data = []
    step_count = 0
    
    while len(collected_data) < target_size:
        # Sample actions and step environments
        actions_list = env_manager.sample_actions()
        experiences = env_manager.step(actions_list)
        
        # Process experiences
        for exp in experiences:
            if len(collected_data) >= target_size:
                break
                
            obs1, action, obs2, reward, done = exp
            collected_data.append({
                'obs1': obs1.copy(),
                'obs2': obs2.copy(), 
                'action': action,
                'reward': reward,
                'done': done
            })
        
        step_count += 1
        if step_count % 10 == 0:
            print(f"  æ”¶é›†è¿›åº¦: {len(collected_data)}/{target_size} æ¡æ•°æ®")
    
    print(f"æ•°æ®æ”¶é›†å®Œæˆï¼å…±æ”¶é›† {len(collected_data)} æ¡æ•°æ®")
    return collected_data[:target_size]  # ç¡®ä¿ç²¾ç¡®æ•°é‡

def save_dataset(dataset, filepath):
    """ä¿å­˜æ•°æ®é›†åˆ°æ–‡ä»¶"""
    with open(filepath, 'wb') as f:
        pickle.dump(dataset, f)
    print(f"æ•°æ®é›†å·²ä¿å­˜åˆ°: {filepath}")

def create_batch_from_dataset(dataset, device):
    """ä»æ•°æ®é›†åˆ›å»ºæ‰¹æ¬¡æ•°æ®"""
    obs1_list = [data['obs1'] for data in dataset]
    obs2_list = [data['obs2'] for data in dataset] 
    actions_list = [data['action'] for data in dataset]
    
    # Preprocess observations
    obs1_batch_bchw = preprocess_observations(obs1_list, device)
    obs2_target_batch_bchw = preprocess_observations(obs2_list, device)
    
    # Reshape for model input (B, L, C, H, W)
    obs1_input_blchw = obs1_batch_bchw.unsqueeze(1)
    obs2_target_blchw = obs2_target_batch_bchw.unsqueeze(1)
    
    # Actions to tensor (B, L)
    actions_tensor_bl = torch.tensor(actions_list, dtype=torch.long, device=device).unsqueeze(1)
    
    return obs1_input_blchw, actions_tensor_bl, obs2_target_blchw

def main():
    print(f"å¼€å§‹è¿‡æ‹Ÿåˆå®éªŒ - è®¾å¤‡: {DEVICE}")
    print("="*60)
    
    # Create directories
    os.makedirs(OVERFIT_MODEL_DIR, exist_ok=True)
    os.makedirs(OVERFIT_IMAGES_DIR, exist_ok=True)
    
    # Initialize log file
    with open(OVERFIT_LOG_FILE, 'w') as f:
        f.write("è¿‡æ‹Ÿåˆè®­ç»ƒæ—¥å¿—\n")
        f.write("="*50 + "\n")
        f.write(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"ç›®æ ‡: åœ¨128æ¡æ•°æ®ä¸Šè¿‡æ‹Ÿåˆï¼ŒæŸå¤±é™è‡³{TARGET_LOSS}ä»¥ä¸‹\n")
        f.write(f"è®¾å¤‡: {DEVICE}\n")
        f.write(f"è®­ç»ƒè¿‡ç¨‹ä¸­å°†ä¿å­˜æ¯ä¸ªç¯å¢ƒä¸‹æŸå¤±æœ€å°çš„4å¼ å¯¹ç…§å›¾\n")
        f.write("="*50 + "\n\n")
    
    # --- Step 1: æ”¶é›†å›ºå®šæ•°æ®é›† ---
    print("æ­¥éª¤ 1: æ”¶é›†å›ºå®šè®­ç»ƒæ•°æ®é›†")
    
    if os.path.exists(OVERFIT_DATA_FILE):
        print(f"å‘ç°å·²å­˜åœ¨çš„æ•°æ®æ–‡ä»¶: {OVERFIT_DATA_FILE}")
        response = input("æ˜¯å¦é‡æ–°æ”¶é›†æ•°æ®ï¼Ÿ(y/n): ").lower().strip()
        if response != 'y':
            print("åŠ è½½å·²å­˜åœ¨çš„æ•°æ®é›†...")
            with open(OVERFIT_DATA_FILE, 'rb') as f:
                fixed_dataset = pickle.load(f)
            print(f"å·²åŠ è½½ {len(fixed_dataset)} æ¡æ•°æ®")
        else:
            # é‡æ–°æ”¶é›†æ•°æ®
            print("é‡æ–°æ”¶é›†æ•°æ®...")
            env_manager = AtariEnvManager(
                num_games=NUM_GAMES_TO_SELECT_FROM, 
                num_envs=NUM_ENVS, 
                render_mode=None
            )
            fixed_dataset = collect_fixed_dataset(env_manager, FIXED_DATA_SIZE)
            save_dataset(fixed_dataset, OVERFIT_DATA_FILE)
            env_manager.close()
    else:
        # æ”¶é›†æ–°æ•°æ®
        env_manager = AtariEnvManager(
            num_games=NUM_GAMES_TO_SELECT_FROM, 
            num_envs=NUM_ENVS, 
            render_mode=None
        )
        fixed_dataset = collect_fixed_dataset(env_manager, FIXED_DATA_SIZE)
        save_dataset(fixed_dataset, OVERFIT_DATA_FILE)
        env_manager.close()
    
    # --- Step 2: åˆå§‹åŒ–æ¨¡å‹ ---
    print("\næ­¥éª¤ 2: åˆå§‹åŒ–æ¨¡å‹")
    
    # ä»ç¬¬ä¸€ä¸ªæ•°æ®æ ·æœ¬è·å–åŠ¨ä½œç»´åº¦
    sample_env_manager = AtariEnvManager(num_games=NUM_GAMES_TO_SELECT_FROM, num_envs=1, render_mode=None)
    action_dim = sample_env_manager.envs[0].action_space.n
    sample_env_manager.close()
    
    model = PredictiveRepModel(
        img_in_channels=IMG_C,
        img_out_channels=IMG_C,
        encoder_layers=ENCODER_LAYERS,
        decoder_layers_config=DECODER_LAYERS,
        target_img_h=IMG_H,
        target_img_w=IMG_W,
        action_dim=action_dim,
        latent_dim=LATENT_DIM
    ).to(DEVICE)
    
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    print(f"æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ŒåŠ¨ä½œç»´åº¦: {action_dim}")
    
    # --- Step 3: è¿‡æ‹Ÿåˆè®­ç»ƒ ---
    print(f"\næ­¥éª¤ 3: å¼€å§‹è¿‡æ‹Ÿåˆè®­ç»ƒ (ç›®æ ‡æŸå¤± < {TARGET_LOSS})")
    print("="*60)
    
    # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
    obs1_input, actions_input, obs2_target = create_batch_from_dataset(fixed_dataset, DEVICE)
    
    # æŒ‰ç¯å¢ƒåˆ†ç±»æ ·æœ¬ï¼Œç”¨äºä¿å­˜æœ€ä½³å¯¹æ¯”å›¾
    print("æŒ‰ç¯å¢ƒåˆ†ç±»æ ·æœ¬...")
    env_samples = classify_samples_by_environment(fixed_dataset)
    for env_name, indices in env_samples.items():
        print(f"  {env_name}: {len(indices)} ä¸ªæ ·æœ¬")
        with open(OVERFIT_LOG_FILE, 'a') as f:
            f.write(f"ç¯å¢ƒ {env_name}: {len(indices)} ä¸ªæ ·æœ¬\n")
    
    epoch = 0
    best_loss = float('inf')
    
    while epoch < MAX_EPOCHS:
        epoch += 1
        
        # è®­ç»ƒä¸€ä¸ªepoch
        loss = model.train_on_batch(obs1_input, actions_input, obs2_target, optimizer)
        
        if loss < best_loss:
            best_loss = loss
        
        # æ‰“å°è¿›åº¦
        if epoch % PRINT_INTERVAL == 0:
            log_message = f"Epoch {epoch:4d}: Loss = {loss:.6f}, Best = {best_loss:.6f}"
            print(log_message)
            
            # è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
            with open(OVERFIT_LOG_FILE, 'a') as f:
                f.write(f"{log_message}\n")
        
        # å®šæœŸè¯„ä¼°å¹¶ä¿å­˜æœ€ä½³æ ·æœ¬å¯¹æ¯”å›¾
        if epoch % EVAL_INTERVAL == 0:
            print(f"  è¯„ä¼°æ¨¡å‹å¹¶ä¿å­˜æœ€ä½³æ ·æœ¬å¯¹æ¯”å›¾ (Epoch {epoch})...")
            avg_eval_loss, saved_images = evaluate_and_save_best_samples(
                model, fixed_dataset, env_samples, epoch, OVERFIT_IMAGES_DIR
            )
            print(f"  è¯„ä¼°å¹³å‡æŸå¤±: {avg_eval_loss:.6f}, ä¿å­˜äº† {saved_images} å¼ å¯¹æ¯”å›¾")
            
            with open(OVERFIT_LOG_FILE, 'a') as f:
                f.write(f"Epoch {epoch} è¯„ä¼°: å¹³å‡æŸå¤±={avg_eval_loss:.6f}, ä¿å­˜å›¾ç‰‡={saved_images}å¼ \n")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % SAVE_INTERVAL == 0:
            checkpoint_path = os.path.join(OVERFIT_MODEL_DIR, f'overfit_epoch_{epoch}.pth')
            torch.save(model.state_dict(), checkpoint_path)
            
            with open(OVERFIT_LOG_FILE, 'a') as f:
                f.write(f"ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}\n")
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡æŸå¤±
        if loss < TARGET_LOSS:
            print(f"\nğŸ‰ è¾¾åˆ°ç›®æ ‡æŸå¤±ï¼")
            print(f"æœ€ç»ˆæŸå¤±: {loss:.6f} < {TARGET_LOSS}")
            print(f"è®­ç»ƒè½®æ•°: {epoch}")
            
            # æœ€ç»ˆè¯„ä¼°å¹¶ä¿å­˜æœ€ä½³æ ·æœ¬
            print("è¿›è¡Œæœ€ç»ˆè¯„ä¼°å¹¶ä¿å­˜æœ€ä½³æ ·æœ¬...")
            final_avg_loss, final_saved_images = evaluate_and_save_best_samples(
                model, fixed_dataset, env_samples, epoch, OVERFIT_IMAGES_DIR
            )
            print(f"æœ€ç»ˆè¯„ä¼°å¹³å‡æŸå¤±: {final_avg_loss:.6f}")
            
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            final_model_path = os.path.join(OVERFIT_MODEL_DIR, 'overfit128.pth')
            torch.save(model.state_dict(), final_model_path)
            print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
            
            # è®°å½•æˆåŠŸä¿¡æ¯
            with open(OVERFIT_LOG_FILE, 'a') as f:
                f.write(f"\nè®­ç»ƒæˆåŠŸå®Œæˆï¼\n")
                f.write(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {loss:.6f}\n")
                f.write(f"æœ€ç»ˆè¯„ä¼°æŸå¤±: {final_avg_loss:.6f}\n")
                f.write(f"è®­ç»ƒè½®æ•°: {epoch}\n")
                f.write(f"æœ€ç»ˆæ¨¡å‹: {final_model_path}\n")
                f.write(f"ä¿å­˜çš„å¯¹æ¯”å›¾æ€»æ•°: {final_saved_images}\n")
                f.write(f"å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            
            break
    
    if epoch >= MAX_EPOCHS:
        print(f"\nâš ï¸  è¾¾åˆ°æœ€å¤§è®­ç»ƒè½®æ•° ({MAX_EPOCHS})ï¼Œæœªè¾¾åˆ°ç›®æ ‡æŸå¤±")
        print(f"å½“å‰æœ€ä½³æŸå¤±: {best_loss:.6f}")
        
        # è¿›è¡Œæœ€ç»ˆè¯„ä¼°
        print("è¿›è¡Œæœ€ç»ˆè¯„ä¼°å¹¶ä¿å­˜æœ€ä½³æ ·æœ¬...")
        final_avg_loss, final_saved_images = evaluate_and_save_best_samples(
            model, fixed_dataset, env_samples, epoch, OVERFIT_IMAGES_DIR
        )
        print(f"æœ€ç»ˆè¯„ä¼°å¹³å‡æŸå¤±: {final_avg_loss:.6f}")
        
        # ä»ç„¶ä¿å­˜æ¨¡å‹
        final_model_path = os.path.join(OVERFIT_MODEL_DIR, 'overfit128_max_epochs.pth')
        torch.save(model.state_dict(), final_model_path)
        print(f"æ¨¡å‹å·²ä¿å­˜: {final_model_path}")
        
        with open(OVERFIT_LOG_FILE, 'a') as f:
            f.write(f"\nè®­ç»ƒè¾¾åˆ°æœ€å¤§è½®æ•°é™åˆ¶\n")
            f.write(f"æœ€ä½³è®­ç»ƒæŸå¤±: {best_loss:.6f}\n")
            f.write(f"æœ€ç»ˆè¯„ä¼°æŸå¤±: {final_avg_loss:.6f}\n")
            f.write(f"æ¨¡å‹ä¿å­˜: {final_model_path}\n")
            f.write(f"ä¿å­˜çš„å¯¹æ¯”å›¾æ€»æ•°: {final_saved_images}\n")
    
    print("\n" + "="*60)
    print("è¿‡æ‹Ÿåˆè®­ç»ƒå®Œæˆï¼")
    print(f"è®­ç»ƒæ•°æ®: {OVERFIT_DATA_FILE}")
    print(f"æ¨¡å‹ä¿å­˜: {OVERFIT_MODEL_DIR}")
    print(f"è®­ç»ƒæ—¥å¿—: {OVERFIT_LOG_FILE}")
    print(f"å¯¹æ¯”å›¾åƒ: {OVERFIT_IMAGES_DIR}")
    print("="*60)

if __name__ == '__main__':
    main()