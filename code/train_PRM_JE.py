# train_PRM_JE.py
import torch.nn.functional as F
import argparse
import logging
import os
import json
import torch
from tqdm import tqdm
import datetime
import csv
import colorama
from colorama import Fore, Back, Style
import sys

# DDP Imports
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler

# Import from other files
from env.AtariDataLoder import create_atari_dataloaders
from model.PRM_JE import PRM_JE, create_optimized_optimizer, create_lr_scheduler

logger = logging.getLogger(__name__)

def print_loss_info(loss_dict, global_step, epoch, batch_idx, lr=None, loss_type="TRAIN"):
    """
    ç”¨å½©è‰²æ‰“å°æŸå¤±ä¿¡æ¯ - é€‚é…PRM_JEçš„ç®€åŒ–æŸå¤±
    """
    color = Fore.RED if loss_type == "TRAIN" else Fore.BLUE
    
    print(f"\n{color}{'='*60}")
    print(f"{color}ğŸ”¥ {loss_type} LOSS INFO - Step {global_step}")
    print(f"{color}{'='*60}")
    print(f"{Fore.YELLOW}Epoch: {epoch+1:3d} | Batch: {batch_idx+1:4d} | Step: {global_step:6d}")
    if lr is not None:
        print(f"{Fore.CYAN}Learning Rate: {lr:.2e}")
    
    print(f"{color}â”Œâ”€ LOSSES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print(f"{color}â”‚ Total Loss:    {loss_dict['total_loss']:10.6f}              â”‚")
    print(f"{color}â”‚ Vector Loss:   {loss_dict['loss_vector']:10.6f}              â”‚")
    print(f"{color}â”‚ Cosine Sim:    {loss_dict['cosine_similarity']:10.6f}              â”‚")
    print(f"{color}â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print(f"{color}{'='*60}{Style.RESET_ALL}")


def setup_ddp():
    """Initializes the DDP process group."""
    dist.init_process_group(backend="nccl")
    rank = int(os.environ['RANK'])
    world_size = int(os.environ['WORLD_SIZE'])
    local_rank = int(os.environ['LOCAL_RANK'])
    torch.cuda.set_device(local_rank)
    return rank, world_size


def cleanup_ddp():
    """Cleans up the DDP process group."""
    dist.destroy_process_group()


def add_all_training_args(parser: argparse.ArgumentParser):
    """æ·»åŠ æ‰€æœ‰è®­ç»ƒç›¸å…³çš„å‚æ•° - é€‚é…PRM_JE"""
    
    # === æ•°æ®è·¯å¾„å’ŒåŠ è½½é…ç½® ===
    data_group = parser.add_argument_group('Data Loading Configuration')
    data_group.add_argument('--data-dir', type=str, default='./Data/replay_data',
                           help='Directory containing PKL files')
    data_group.add_argument('--num-workers', type=int, default=4,
                           help='Number of data loading worker processes')
    
    # === æ•°æ®é¢„å¤„ç†é…ç½® ===
    preprocess_group = parser.add_argument_group('Data Preprocessing Configuration')
    preprocess_group.add_argument('--img-height', type=int, default=210,
                                 help='Target image height')
    preprocess_group.add_argument('--img-width', type=int, default=160,
                                 help='Target image width')
    
    # === åºåˆ—å’Œæ‰¹æ¬¡é…ç½® ===
    sequence_group = parser.add_argument_group('Sequence and Batch Configuration')
    sequence_group.add_argument('--sequence-length', type=int, default=32,
                               help='Length of each training sequence (model will see sequence_length+1 frames)')
    sequence_group.add_argument('--batch-size', type=int, default=8,
                               help='Number of sequences per batch')
    sequence_group.add_argument('--overlap-steps', type=int, default=16,
                               help='Number of steps to move forward when creating overlapping sequences')
    
    # === æ•°æ®é›†åˆ’åˆ†é…ç½® ===
    split_group = parser.add_argument_group('Dataset Split Configuration')
    split_group.add_argument('--train-split', type=float, default=0.8,
                            help='Proportion of episodes for training')
    split_group.add_argument('--val-split', type=float, default=0.1,
                            help='Proportion of episodes for validation')
    
    # === è®­ç»ƒé…ç½® ===
    train_group = parser.add_argument_group('Training Configuration')
    train_group.add_argument('--epochs', type=int, default=50, 
                            help='Number of training epochs')
    train_group.add_argument('--output-dir', type=str, default='./Output', 
                            help='Base directory to save all training outputs')
    train_group.add_argument('--run-name', type=str, default='prm_je_episode_100', 
                            help='A specific name for this training run')
    train_group.add_argument('--log-interval', type=int, default=50, 
                            help='How often (in batches) to log to CSV')

    # === PRM_JE æ¨¡å‹é…ç½® ===
    model_group = parser.add_argument_group('PRM_JE Model Configuration')
    model_group.add_argument('--latent-dim', type=int, default=256, 
                            help='Dimension of the latent space')
    model_group.add_argument('--base-channels', type=int, default=64, 
                            help='Base number of channels for convolutional layers')
    model_group.add_argument('--transformer-layers', type=int, default=3, 
                            help='Number of layers in the Transformer predictor')
    model_group.add_argument('--num-attention-heads', type=int, default=8, 
                            help='Number of attention heads in the Transformer')
    model_group.add_argument('--loss-weight', type=float, default=1.0, 
                            help='Weight for vector loss (only one loss in PRM_JE)')
    
    # === ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨é…ç½® ===
    optim_group = parser.add_argument_group('Optimizer and Scheduler Configuration')
    optim_group.add_argument('--lr', type=float, default=1e-4, 
                            help='Base learning rate')
    optim_group.add_argument('--weight-decay', type=float, default=1e-2, 
                            help='Weight decay for the AdamW optimizer')
    optim_group.add_argument('--warmup-steps', type=int, default=1000, 
                            help='Number of warmup steps for the learning rate scheduler')
    optim_group.add_argument('--grad-clip-norm', type=float, default=1.0, 
                            help='Maximum norm for gradient clipping')
    
    # === è°ƒè¯•å’Œæ—¥å¿—é…ç½® ===
    debug_group = parser.add_argument_group('Debug and Logging Configuration')
    debug_group.add_argument('--log-level', type=str, default='INFO',
                            choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                            help='Logging level')
    debug_group.add_argument('--max-episodes', type=int, default=None,
                            help='Maximum number of episodes to use (for debugging)')


def train_epoch(model, dataloader, optimizer, scheduler, device, args, epoch, csv_writer, csv_file, rank):
    """
    ä¿®æ”¹åçš„è®­ç»ƒå‡½æ•°ï¼šé€‚é…PRM_JEæ¨¡å‹
    """
    model.train()
    progress_bar = tqdm(dataloader, desc=f"Training Epoch {epoch+1}", leave=False, disable=(rank != 0))

    batch_count = 0
    last_batch_data = None  # ä¿å­˜æœ€åä¸€ä¸ªbatchç”¨äºè¯„ä¼°
    
    # åŠ¨æ€è°ƒæ•´è¯„ä¼°é¢‘ç‡
    total_batches = len(dataloader)
    eval_interval = min(64, max(10, total_batches // 4))  # è¯„ä¼°é—´éš”ï¼šæœ€å°10ï¼Œæœ€å¤§64ï¼Œæˆ–æ€»batchæ•°çš„1/4
    log_interval = min(args.log_interval, max(5, total_batches // 8))  # æ—¥å¿—é—´éš”ï¼šè°ƒæ•´ä¸ºæ›´åˆç†çš„å€¼
    
    if rank == 0:
        print(f"{Fore.CYAN}ğŸ“Š Training with {total_batches} batches per epoch")
        print(f"{Fore.CYAN}ğŸ“Š Evaluation every {eval_interval} batches")
        print(f"{Fore.CYAN}ğŸ“Š Regular logging every {log_interval} batches{Style.RESET_ALL}")
    
    for batch_idx, batch in enumerate(progress_bar):
        global_step = epoch * len(dataloader) + batch_idx
        batch_count += 1
        
        optimizer.zero_grad()
        obs = batch['observations'].to(device, non_blocking=True)
        actions = batch['actions'].to(device, non_blocking=True)
        # æ³¨æ„ï¼šPRM_JEä¸éœ€è¦doneæ ‡ç­¾
        
        B, T_plus_1, C, H, W = obs.shape
        T = T_plus_1 - 1
        
        input_images = obs[:, :-1, ...].reshape(B*T, C, H, W)
        input_actions = actions[:, :-1].reshape(B*T)
        target_images = obs[:, 1:, ...].reshape(B*T, C, H, W)

        # PRM_JEåªè¿”å›ä¸¤ä¸ªè¾“å‡º
        encoder_features, predicted_features = model(input_images, input_actions)
        
        with torch.no_grad():
            target_features = model.module.encoder(target_images)
            
        loss, loss_dict = model.module.compute_loss(predicted_features, target_features)
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=args.grad_clip_norm)
        optimizer.step()
        scheduler.step()

        # ä¿å­˜å½“å‰batchæ•°æ®ç”¨äºè¯„ä¼°
        last_batch_data = batch

        if rank == 0:
            current_lr = optimizer.param_groups[0]['lr']
            
            # æ›´æ–°è¿›åº¦æ¡ - ç®€åŒ–çš„æ˜¾ç¤º
            progress_bar.set_postfix({
                'loss': f"{loss.item():.4f}",
                'vec': f"{loss_dict['loss_vector']:.4f}",
                'cos_sim': f"{loss_dict['cosine_similarity']:.4f}",
                'lr': f"{current_lr:.2e}"
            })
            
            # ä½¿ç”¨åŠ¨æ€è°ƒæ•´çš„è¯„ä¼°é¢‘ç‡
            if batch_count % eval_interval == 0:
                # è®­ç»ƒæŸå¤±ä¿¡æ¯ï¼ˆçº¢è‰²ï¼‰
                print_loss_info(loss_dict, global_step + 1, epoch, batch_idx, current_lr, "TRAIN")
                
                # åœ¨å½“å‰batchä¸Šè¿›è¡Œè¯„ä¼°ï¼ˆè“è‰²ï¼‰
                eval_loss_dict = evaluate_on_current_batch(model, last_batch_data, device)
                print_loss_info(eval_loss_dict, global_step + 1, epoch, batch_idx, current_lr, "EVAL")
                
                # å†™å…¥CSV - è®­ç»ƒå’Œè¯„ä¼°æŸå¤±éƒ½è®°å½•
                if csv_writer is not None and csv_file is not None:
                    try:
                        # è®­ç»ƒæŸå¤±è¡Œ
                        train_log_data = [
                            global_step + 1,
                            "train",
                            loss_dict['total_loss'],
                            loss_dict['loss_vector'],
                            loss_dict.get('cosine_similarity', 0.0),
                            current_lr
                        ]
                        csv_writer.writerow(train_log_data)
                        
                        # è¯„ä¼°æŸå¤±è¡Œ
                        eval_log_data = [
                            global_step + 1,
                            "eval",
                            eval_loss_dict['total_loss'],
                            eval_loss_dict['loss_vector'],
                            eval_loss_dict.get('cosine_similarity', 0.0),
                            current_lr
                        ]
                        csv_writer.writerow(eval_log_data)
                        
                        # å¼ºåˆ¶åˆ·æ–°æ–‡ä»¶
                        csv_file.flush()
                        
                    except Exception as e:
                        print(f"{Fore.RED}CSVå†™å…¥é”™è¯¯: {e}{Style.RESET_ALL}")
            
            # ä½¿ç”¨åŠ¨æ€è°ƒæ•´çš„æ—¥å¿—é¢‘ç‡
            elif (batch_idx + 1) % log_interval == 0 and csv_writer is not None and csv_file is not None:
                try:
                    regular_log_data = [
                        global_step + 1,
                        "train",
                        loss_dict['total_loss'],
                        loss_dict['loss_vector'],
                        loss_dict.get('cosine_similarity', 0.0),
                        current_lr
                    ]
                    csv_writer.writerow(regular_log_data)
                    csv_file.flush()
                except Exception as e:
                    print(f"{Fore.RED}CSVå†™å…¥é”™è¯¯: {e}{Style.RESET_ALL}")
            
            # ç¡®ä¿æ¯ä¸ªepochè‡³å°‘è®°å½•ä¸€æ¬¡ï¼ˆæœ€åä¸€ä¸ªbatchï¼‰
            elif batch_idx == len(dataloader) - 1 and csv_writer is not None and csv_file is not None:
                try:
                    final_log_data = [
                        global_step + 1,
                        "train_final",
                        loss_dict['total_loss'],
                        loss_dict['loss_vector'],
                        loss_dict.get('cosine_similarity', 0.0),
                        current_lr
                    ]
                    csv_writer.writerow(final_log_data)
                    csv_file.flush()
                    print(f"{Fore.GREEN}ğŸ“ Epoch {epoch+1} final batch logged{Style.RESET_ALL}")
                except Exception as e:
                    print(f"{Fore.RED}CSVå†™å…¥é”™è¯¯: {e}{Style.RESET_ALL}")


def evaluate_on_current_batch(model, batch, device):
    """
    åœ¨å½“å‰batchä¸Šè¿›è¡Œè¯„ä¼°ï¼ˆé€‚é…PRM_JEï¼‰
    """
    model.eval()
    
    with torch.no_grad():
        obs = batch['observations'].to(device, non_blocking=True)
        actions = batch['actions'].to(device, non_blocking=True)
        
        B, T_plus_1, C, H, W = obs.shape
        T = T_plus_1 - 1
        
        input_images = obs[:, :-1, ...].reshape(B*T, C, H, W)
        input_actions = actions[:, :-1].reshape(B*T)
        target_images = obs[:, 1:, ...].reshape(B*T, C, H, W)

        # PRM_JEåªè¿”å›ä¸¤ä¸ªè¾“å‡º
        encoder_features, predicted_features = model(input_images, input_actions)
        
        target_features = model.module.encoder(target_images)
            
        loss, loss_dict = model.module.compute_loss(predicted_features, target_features)
    
    model.train()  # åˆ‡å›è®­ç»ƒæ¨¡å¼
    return loss_dict


def validate_epoch(model, dataloader, device):
    """æ‰§è¡Œä¸€ä¸ªéªŒè¯epochï¼ˆé€‚é…PRM_JEï¼‰"""
    model.eval()
    total_val_loss = 0
    all_loss_dicts = []
    
    with torch.no_grad():
        progress_bar = tqdm(dataloader, desc="Validating", leave=False)
        for batch in progress_bar:
            obs = batch['observations'].to(device)
            actions = batch['actions'].to(device)

            B, T_plus_1, C, H, W = obs.shape
            T = T_plus_1 - 1

            input_images = obs[:, :-1, ...].reshape(B*T, C, H, W)
            input_actions = actions[:, :-1].reshape(B*T)
            target_images = obs[:, 1:, ...].reshape(B*T, C, H, W)

            encoder_features, predicted_features = model(input_images, input_actions)
            target_features = model.encoder(target_images)
            
            _, loss_dict = model.compute_loss(predicted_features, target_features)
            
            all_loss_dicts.append(loss_dict)
            total_val_loss += loss_dict['total_loss']
    
    # è®¡ç®—æ‰€æœ‰éªŒè¯æ‰¹æ¬¡çš„å¹³å‡æŒ‡æ ‡
    avg_metrics = {k: sum(d[k] for d in all_loss_dicts) / len(all_loss_dicts) for k in all_loss_dicts[0]}
    return total_val_loss / len(dataloader), avg_metrics


def main():
    """
    ä¿®æ”¹åçš„ä¸»å‡½æ•° - é€‚é…PRM_JE
    """
    parser = argparse.ArgumentParser(description="Train a PRM_JE with DDP and unified logging.")
    add_all_training_args(parser)
    args = parser.parse_args()

    logging.getLogger().setLevel(getattr(logging, args.log_level))

    rank, world_size = setup_ddp()
    device = torch.device(f"cuda:{rank}")

    csv_writer, csv_file, run_path = None, None, None

    if rank == 0:
        run_path = os.path.join(args.output_dir, args.run_name)
        os.makedirs(run_path, exist_ok=True)
        
        log_file_path = os.path.join(run_path, 'training.log')
        logging.basicConfig(
            level=getattr(logging, args.log_level), 
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[logging.FileHandler(log_file_path), logging.StreamHandler()]
        )
        
        # CSVæ–‡ä»¶åˆ›å»º - ç®€åŒ–çš„åˆ—ç»“æ„é€‚é…PRM_JE
        csv_file_path = os.path.join(run_path, 'loss_log.csv')
        csv_file = open(csv_file_path, 'w', newline='', encoding='utf-8')
        csv_writer = csv.writer(csv_file)
        
        # ç®€åŒ–çš„CSVè¡¨å¤´ï¼ŒåªåŒ…å«PRM_JEç›¸å…³çš„æŒ‡æ ‡
        csv_writer.writerow([
            'global_step', 'mode', 'total_loss', 'loss_vector', 'cosine_similarity', 'learning_rate'
        ])
        csv_file.flush()  # ç«‹å³å†™å…¥è¡¨å¤´

        print(f"{Fore.GREEN}{'='*60}")
        print(f"{Fore.GREEN}ğŸš€ Starting PRM_JE DDP Training")
        print(f"{Fore.GREEN}{'='*60}")
        print(f"{Fore.CYAN}Model: PRM_JE (Encoder + Predictive Model only)")
        print(f"{Fore.CYAN}Run name: {args.run_name}")
        print(f"{Fore.CYAN}Output dir: {run_path}")
        print(f"{Fore.CYAN}GPUs: {world_size}")
        print(f"{Fore.CYAN}Batch size per GPU: {args.batch_size}")
        print(f"{Fore.CYAN}Sequence length: {args.sequence_length}")
        print(f"{Fore.CYAN}Effective batch size: {args.batch_size * args.sequence_length * world_size}")
        print(f"{Fore.YELLOW}Loss: Only cosine similarity loss for feature prediction")
        print(f"{Fore.YELLOW}Evaluation: Every 64 batches on current batch")
        print(f"{Fore.GREEN}{'='*60}{Style.RESET_ALL}")
        
        with open(os.path.join(run_path, 'config.json'), 'w') as f:
            json.dump(vars(args), f, indent=4)

    # ä½¿ç”¨åŸæœ‰çš„æ•°æ®åŠ è½½å™¨ï¼ˆä¸ä¿®æ”¹ï¼‰
    train_loader, val_loader = create_atari_dataloaders(args)
    
    if train_loader is None:
        logger.error("Failed to create train loader. Exiting.")
        cleanup_ddp()
        return
    
    # ä¸º DDP åˆ›å»ºåˆ†å¸ƒå¼é‡‡æ ·å™¨
    train_dataset = train_loader.dataset
    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)
    train_loader = torch.utils.data.DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=False,
        num_workers=args.num_workers, 
        pin_memory=True, 
        sampler=train_sampler, 
        drop_last=True
    )
    
    if rank == 0:
        if val_loader is not None:
            print(f"{Fore.GREEN}âœ… Created dataloaders - Train: {len(train_loader)}, Val: {len(val_loader)}{Style.RESET_ALL}")
            print(f"{Fore.YELLOW}ğŸ“ Note: Using current batch evaluation instead of separate validation{Style.RESET_ALL}")
        else:
            print(f"{Fore.YELLOW}âœ… Created dataloaders - Train: {len(train_loader)} (no validation){Style.RESET_ALL}")

    # åˆ›å»ºPRM_JEæ¨¡å‹
    model = PRM_JE(
        img_in_channels=3,
        encoder_layers=[2, 3, 4, 3],
        action_dim=19,  # 0-18å…±19ä¸ªåŠ¨ä½œ
        latent_dim=args.latent_dim,
        base_channels=args.base_channels,
        num_attention_heads=args.num_attention_heads,
        transformer_layers=args.transformer_layers,
        loss_weight=args.loss_weight,
        dropout=0.1
    ).to(device)
    
    model = DDP(model, device_ids=[rank], find_unused_parameters=False)
    optimizer = create_optimized_optimizer(model, base_lr=args.lr, weight_decay=args.weight_decay)
    max_steps = args.epochs * len(train_loader)
    scheduler = create_lr_scheduler(optimizer, warmup_steps=args.warmup_steps, max_steps=max_steps)

    if rank == 0:
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"{Fore.CYAN}ğŸ“Š Model Parameters: {total_params:,} total, {trainable_params:,} trainable{Style.RESET_ALL}")

    # è®­ç»ƒå¾ªç¯
    best_val_loss = float('inf')
    for epoch in range(args.epochs):
        train_sampler.set_epoch(epoch)
        
        if rank == 0:
            print(f"\n{Fore.MAGENTA}{'='*60}")
            print(f"{Fore.MAGENTA}ğŸ¯ EPOCH {epoch+1}/{args.epochs}")
            print(f"{Fore.MAGENTA}{'='*60}{Style.RESET_ALL}")
        
        # ä½¿ç”¨ä¿®æ”¹åçš„è®­ç»ƒå‡½æ•°
        train_epoch(model, train_loader, optimizer, scheduler, device, args, epoch, csv_writer, csv_file, rank)
        
        if rank == 0:
            # ä¿å­˜æ¨¡å‹
            checkpoint_path = os.path.join(run_path, f'model_epoch_{epoch+1}.pth')
            torch.save(model.module.state_dict(), checkpoint_path)
            print(f"{Fore.GREEN}ğŸ’¾ Epoch {epoch+1} model saved!{Style.RESET_ALL}")

    if rank == 0:
        final_checkpoint_path = os.path.join(run_path, 'final_model.pth')
        torch.save(model.module.state_dict(), final_checkpoint_path)
        print(f"\n{Fore.GREEN}{'='*60}")
        print(f"{Fore.GREEN}ğŸ‰ PRM_JE TRAINING COMPLETED!")
        print(f"{Fore.GREEN}{'='*60}")
        print(f"{Fore.GREEN}Final model saved: {final_checkpoint_path}")
        print(f"{Fore.GREEN}Loss log saved: {csv_file.name}")
        print(f"{Fore.GREEN}Model type: PRM_JE (Encoder + Predictive Model)")
        print(f"{Fore.GREEN}{'='*60}{Style.RESET_ALL}")
        
        if csv_file:
            csv_file.close()
    
    cleanup_ddp()


if __name__ == '__main__':
    main()
